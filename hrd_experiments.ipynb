{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1WhlRE9VMTTo",
        "5CdRCfOmMWSG",
        "OywRfxxeWNT1",
        "i_KD33-GwJ9T",
        "VyCKfHFG-UiF"
      ],
      "mount_file_id": "1gB6idZ6vnQ_52FH7caEId1lfSyqac2jS",
      "authorship_tag": "ABX9TyOWRtswoygi5QgHknZjoP4K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranay8297/Harvard-Rare-Disease-Hackathon/blob/main/hrd_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Prep"
      ],
      "metadata": {
        "id": "1WhlRE9VMTTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pronto"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFKKxow-GJVz",
        "outputId": "005cd84a-0f11-40b4-b924-561a1c0b6da8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pronto\n",
            "  Downloading pronto-2.6.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: chardet~=5.0 in /usr/local/lib/python3.11/dist-packages (from pronto) (5.2.0)\n",
            "Collecting fastobo~=0.13.0 (from pronto)\n",
            "  Downloading fastobo-0.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: networkx<4.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from pronto) (3.4.2)\n",
            "Requirement already satisfied: python-dateutil~=2.8 in /usr/local/lib/python3.11/dist-packages (from pronto) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil~=2.8->pronto) (1.17.0)\n",
            "Downloading pronto-2.6.0-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastobo-0.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fastobo, pronto\n",
            "Successfully installed fastobo-0.13.0 pronto-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqbL2A-DGAsx"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import pronto"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pheno_ontology = pronto.Ontology('./hpo.obo')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1F9MN1lGEu2",
        "outputId": "cdf5c64d-42cb-4ad4-ab8d-a0a68c2bfe45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-445b7bd13fd2>:1: UnicodeWarning: unsound encoding, assuming ISO-8859-1 (73% confidence)\n",
            "  pheno_ontology = pronto.Ontology('./hpo.obo')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hpo = pheno_ontology\n",
        "pheno = hpo.get_term('HP:0000059')"
      ],
      "metadata": {
        "id": "8wEnVgC9Gfj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ancestors_up_to_root(ontology, start_term, stop_term='HP:0000118'):\n",
        "    \"\"\"\n",
        "    :param ontology: pronto Ontology object (computed in function \"load_ontology\")\n",
        "    :param start_term: specific HPO term (e.g., 'HP:0000164')\n",
        "    :param stop_term: root term, known to be 'HP:0000118' (phenotypic abnormality)\n",
        "    :return: set of all parent terms up to the root term\n",
        "    \"\"\"\n",
        "\n",
        "    ancestors = set()\n",
        "    current_term = ontology[start_term]\n",
        "\n",
        "    for parent in current_term.superclasses():\n",
        "        if parent.id == current_term.id: # first item is always the term itself\n",
        "            continue\n",
        "        if parent.id == stop_term:\n",
        "            break\n",
        "        ancestors.add(parent.id)\n",
        "\n",
        "    return ancestors"
      ],
      "metadata": {
        "id": "in_kHmdrIBe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_disease_annotations(hpo_disease_annotations):\n",
        "    \"\"\"\n",
        "    :param hpo_disease_annotations: full path to the tab-delimited \"phenotype.hpoa\" file downloaded from HPO\n",
        "    :return: dictionary from disease ID -> set of corresponding HPO terms AND dictionary from disease ID -> name\n",
        "    \"\"\"\n",
        "\n",
        "    disease_to_hpo = {}\n",
        "    disease_to_name = {}\n",
        "\n",
        "    ontology = pronto.Ontology('./hpo.obo')\n",
        "\n",
        "    with open(hpo_disease_annotations, 'r') as anno_handle:\n",
        "        header = None\n",
        "        for anno_line in anno_handle:\n",
        "\n",
        "            if anno_line.startswith('#'):\n",
        "                continue\n",
        "\n",
        "            if not header:\n",
        "                header = anno_line.strip().split('\\t')\n",
        "                continue\n",
        "\n",
        "            disease_id, disease_name, _, hpo_id = anno_line.split('\\t')[0:4]\n",
        "\n",
        "            if disease_id not in disease_to_hpo:  # create dictionary of disease -> set (HPO terms)\n",
        "                disease_to_hpo[disease_id] = set()\n",
        "                disease_to_name[disease_id] = disease_name\n",
        "\n",
        "            disease_to_hpo[disease_id].add(hpo_id)\n",
        "\n",
        "            # get all parents and add it to the same disease\n",
        "            parents = get_ancestors_up_to_root(ontology, hpo_id)\n",
        "            for parent in parents:\n",
        "                disease_to_hpo[disease_id].add(parent)\n",
        "\n",
        "    # storing it as a json file here\n",
        "    import json\n",
        "    new_d = {}\n",
        "    for k, v in disease_to_hpo.items():\n",
        "        new_d[k] = list(v)\n",
        "    with open('./disease_to_hpo.json', 'w') as fp:\n",
        "        json.dump(new_d, fp)\n",
        "\n",
        "    print(\"Number of diseases with annotations = \" + str(len(disease_to_hpo.keys())))\n",
        "    print(\"Average number terms/disease = \" + str(\n",
        "        sum([len(v) for v in disease_to_hpo.values()]) / len(disease_to_hpo.keys())))\n",
        "\n",
        "    return disease_to_hpo, disease_to_name"
      ],
      "metadata": {
        "id": "c6J5viE6HEnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "read_disease_annotations('/content/phenotype.hpoa')"
      ],
      "metadata": {
        "id": "SghyYOfaIciJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('/content/disease_to_hpo.json', 'r') as f:\n",
        "    data = json.load(f)"
      ],
      "metadata": {
        "id": "AiubBCsHOrpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 0\n",
        "for i in data.values():\n",
        "    if len(i) > max_len:\n",
        "        max_len = len(i)\n",
        "print(max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izetW6sKPALA",
        "outputId": "e9febf8a-fd8e-4f21-fde5-fa6035def0c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer, Dataset and DataLoader"
      ],
      "metadata": {
        "id": "5CdRCfOmMWSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "'''\n",
        "X - Phenotypes\n",
        "Y - Disease\n",
        "\n",
        "Approach - Sample random phenotypes from X (30 pct)\n",
        "Y - Disease\n",
        "\n",
        "Model - Transformer\n",
        "Sequential Data\n",
        "\n",
        "X - already a list of phenotypes\n",
        "Conver this X to Embeddings\n",
        "1. Add a cls token embedding at the start\n",
        "2. Pad them for the symptom length\n",
        "No positional encoding\n",
        "\n",
        "Model:\n",
        "\n",
        "Transformer\n",
        "1. Masked Attention Needs to be done and avoid all pad tokens\n",
        "2. Multi head Attention\n",
        "\n",
        "Loss Function: Cross Entropy Loss\n",
        "Optimizer: AdamW\n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "Model Initialization: Kaiming Init - Or, lets experiment - With random variances, the key that final activation should have\n",
        "a standard normal distribution\n",
        "'''"
      ],
      "metadata": {
        "id": "0Pnsq53ZPkqv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "c55c1ed1-7584-4841-a083-21674a9a8805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nX - Phenotypes\\nY - Disease\\n\\nApproach - Sample random phenotypes from X (30 pct)\\nY - Disease\\n\\nModel - Transformer\\nSequential Data\\n\\nX - already a list of phenotypes\\nConver this X to Embeddings\\n1. Add a cls token embedding at the start\\n2. Pad them for the symptom length\\nNo positional encoding\\n\\nModel:\\n\\nTransformer\\n1. Masked Attention Needs to be done and avoid all pad tokens\\n2. Multi head Attention\\n\\nLoss Function: Cross Entropy Loss\\nOptimizer: AdamW\\n\\n\\n---- \\n\\nModel Initialization: Kaiming Init - Or, lets experiment - With random variances, the key that final activation should have \\na standard normal distribution\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pickle\n",
        "import random\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "from einops import rearrange\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "b_FnX5F_BTzq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer():\n",
        "    def __init__(self, pheno_list, tokenizer_path = None, max_len = 256, save_path = None):\n",
        "\n",
        "        if tokenizer_path:\n",
        "            try:\n",
        "                with open(tokenizer_path, 'rb') as f:\n",
        "                    self = pickle.load(f)\n",
        "                    return\n",
        "            except:\n",
        "                print('TOKENIZER NOT FOUND, INITIALIZING NEW ONE')\n",
        "\n",
        "        self.key_value = {\n",
        "            'CLS_KEY': 0,\n",
        "            'PAD_KEY': 1,\n",
        "            'UNK': 2\n",
        "        }\n",
        "\n",
        "        self.max_len = max_len\n",
        "        self.phenotype_list = pheno_list\n",
        "        for idx, pheno in enumerate(pheno_list):\n",
        "            self.key_value[pheno] = idx + 3\n",
        "\n",
        "        self.value_key = {v: k for k, v in self.key_value.items()}\n",
        "        if save_path:\n",
        "            self.save_tokenizer(save_path)\n",
        "\n",
        "    def get_key(self, pheno):\n",
        "        try:\n",
        "            return self.key_value[pheno]\n",
        "        except:\n",
        "            return self.key_value['UNK']\n",
        "\n",
        "    def save_tokenizer(self, tokenizer_path):\n",
        "        print(f'SAVING TOKENIZER AT: {tokenizer_path}')\n",
        "        with open(tokenizer_path, 'wb') as f:\n",
        "            pickle.dump(self, f)\n",
        "\n",
        "    def _tokenize(self, pheno_list):\n",
        "        return [self.key_value[pheno] if pheno in self.key_value else self.key_value['UNK'] for pheno in pheno_list]\n",
        "\n",
        "    def pad(self, pheno_list):\n",
        "        pheno_list = ['CLS_KEY'] + pheno_list\n",
        "        delta = max(0, (self.max_len - len(pheno_list)))\n",
        "        if len(pheno_list) < self.max_len:\n",
        "            pheno_list += ['PAD_KEY'] * (self.max_len - len(pheno_list))\n",
        "        else:\n",
        "            pheno_list = pheno_list[:self.max_len]\n",
        "\n",
        "        assert len(pheno_list) == self.max_len\n",
        "        return pheno_list, delta\n",
        "\n",
        "    def tokenize(self, pheno_list):\n",
        "        # 2 things\n",
        "        # 1. Tokenized Pheno List\n",
        "        # 2. Pad Mask\n",
        "        padded_pheno_list, n_padded = self.pad(pheno_list)\n",
        "        tokenized_pheno_list = torch.tensor(self._tokenize(padded_pheno_list)).to(torch.long)\n",
        "        pad_mask = torch.tensor([1] * (self.max_len - n_padded) + [0] * n_padded).to(torch.long)\n",
        "\n",
        "        # Verification\n",
        "        assert len(tokenized_pheno_list) == self.max_len\n",
        "        assert len(pad_mask) == self.max_len\n",
        "\n",
        "        return tokenized_pheno_list, pad_mask\n",
        "\n",
        "    def reverse_tokenize(self, tokenized_pheno_list):\n",
        "        return [self.value_key[token] for token in tokenized_pheno_list]"
      ],
      "metadata": {
        "id": "wt9o2mSPC6-4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(pheno_list, max_len = 256, save_path = '/content/tokenizer.pkl')\n",
        "# tokenizer = Tokenizer(pheno_list, tokenizer_path = '/content/tokenizer.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deWMNAbuMi8N",
        "outputId": "4e0b220f-2104-4547-d1c7-22c948458362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SAVING TOKENIZER AT: /content/tokenizer.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = list(data.values())[0]\n",
        "len(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeR_86w9Ngb7",
        "outputId": "2f402a5e-36d4-4754-97c7-484db8b8ad37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class DS(Dataset):\n",
        "    def __init__(self, data_dict, tokenizer, save_path = None):\n",
        "        self.data = data_dict\n",
        "\n",
        "        self.X = list(data_dict.values())\n",
        "        self.Y = list(data_dict.keys())\n",
        "        self.Y_values = {y: i for i, y in enumerate(self.Y)}\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        if save_path:\n",
        "            self.save_dataset(save_path)\n",
        "\n",
        "    def save_dataset(self, save_path):\n",
        "        print(f'SAVING DATASET AT: {save_path}')\n",
        "        with open(save_path, 'wb') as f:\n",
        "            pickle.dump(self, f)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx]\n",
        "        y = self.Y[idx]\n",
        "\n",
        "        if len(x) > self.tokenizer.max_len:\n",
        "            # sample tokenizer.max_len number of items out of this list randomly\n",
        "            sampled_indices = random.sample(range(len(x)), self.tokenizer.max_len)\n",
        "            x = [x[i] for i in sampled_indices]\n",
        "\n",
        "        elif len(x) < self.tokenizer.max_len:\n",
        "            # sample somewhere between 30% to 70% of x randomly\n",
        "            sample_percentage = random.uniform(0.2, 0.99)\n",
        "            sample_size = max(1, int(len(x) * sample_percentage))\n",
        "            sampled_indices = random.sample(range(len(x)), sample_size)\n",
        "            x = [x[i] for i in sampled_indices]\n",
        "\n",
        "        tokens, mask = self.tokenizer.tokenize(x)\n",
        "        return (tokens, mask), self.Y_values[y]"
      ],
      "metadata": {
        "id": "cTUqDhEfOelJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = DS(data, tokenizer)\n",
        "# with open('/content/dataset.pkl', 'rb') as f:\n",
        "#     dataset = pickle.load(f)\n",
        "dataloader = DataLoader(dataset, batch_size = 64, shuffle = True)"
      ],
      "metadata": {
        "id": "7k7zRDhfQzCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YAY Dataset is working and verified\n",
        "(tokens, mask), disease = dataset[0]"
      ],
      "metadata": {
        "id": "I-5KSXKNTSYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the Dataset\n",
        "with open('/content/dataset.pkl', 'wb') as f:\n",
        "    pickle.dump(dataset, f)"
      ],
      "metadata": {
        "id": "1m3yghk_uatA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_data = iter(dataloader)\n",
        "x, y = next(iter_data)"
      ],
      "metadata": {
        "id": "MyGrCPxlT7w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[0].shape, x[1].shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMpWh9dlVQnj",
        "outputId": "7ee45167-ae8b-46d6-cac5-4b4b919a2db2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 256]), torch.Size([64, 256]), torch.Size([64]))"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.key_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6ruvL7_k1W5",
        "outputId": "46e87163-e5b8-4751-e8b8-4de243eb25d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12508"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lets Write the Transformer MANNNNN!!!!!!"
      ],
      "metadata": {
        "id": "OywRfxxeWNT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(module):\n",
        "    if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "        # Initialize linear and embedding weights with Kaiming initialization\n",
        "        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n",
        "        if hasattr(module, 'bias') and module.bias is not None:\n",
        "            nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n",
        "        # Initialize normalization layer parameters\n",
        "        if module.weight is not None:\n",
        "            nn.init.ones_(module.weight)\n",
        "        if module.bias is not None:\n",
        "            nn.init.zeros_(module.bias)\n",
        "\n",
        "class MultiHeadAttention(nn.Module): # Verified\n",
        "\n",
        "    def __init__(self, d_model, nhead):\n",
        "\n",
        "        super().__init__()\n",
        "        assert d_model % nhead == 0\n",
        "\n",
        "        self.kqv = nn.Linear(d_model, 3*d_model)\n",
        "        self.proj = nn.Linear(d_model, d_model)\n",
        "        self.nhead = nhead\n",
        "\n",
        "    def forward(self, x, attention_mask):\n",
        "\n",
        "        # x -> (b, s, e) -> (b s, h, e/h)\n",
        "        kqv = self.kqv(x) # (b, s, 3*d_model)\n",
        "        k, q, v = torch.chunk(kqv, 3, dim = -1) # (3, b, s, d_model)\n",
        "\n",
        "        k = rearrange(k, 'b s (h e) -> b h s e', h = self.nhead)\n",
        "        q = rearrange(q, 'b s (h e) -> b h s e', h = self.nhead)\n",
        "        v = rearrange(v, 'b s (h e) -> b h s e', h = self.nhead)\n",
        "\n",
        "        # Attention claculation - # TODO: make is_casual true in case of finetuning - Very important\n",
        "        y = F.scaled_dot_product_attention(q, k, v, attn_mask = attention_mask[:, None, None, :], is_causal = False) # flash attention\n",
        "        y = rearrange(y, 'b h s e -> b s (h e)', h = self.nhead)\n",
        "\n",
        "        return self.proj(y)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(d_model, 4 * d_model),\n",
        "            nn.GELU(approximate = 'tanh'),\n",
        "            nn.Linear(4 * d_model, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead):\n",
        "        # Self Attn\n",
        "        # MLP\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, nhead)\n",
        "        self.mlp = MLP(d_model)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn = self.self_attn(x, mask)\n",
        "        x = self.norm1(x + attn)\n",
        "        mlp_out = self.mlp(x)\n",
        "        x = self.norm2(x + mlp_out)\n",
        "        return x\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim = 512, n_classes = 12687, n_layers = 8, n_heads = 16):\n",
        "\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.norm = nn.LayerNorm(emb_dim)\n",
        "        self.transformers = nn.ModuleList([EncoderLayer(d_model = emb_dim, nhead = n_heads) for _ in range(n_layers)])\n",
        "        self.classifier = nn.Linear(emb_dim, n_classes)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # breakpoint()\n",
        "        x = self.embedding(x) # (b, s, e)\n",
        "        x = self.norm(x) # (b, s, e)\n",
        "        mask = mask.bool()\n",
        "        for transformer in self.transformers:\n",
        "            x = transformer(x, mask)\n",
        "\n",
        "        x = x[:, 0, :].squeeze() # (b, s, e) -> (b, e)\n",
        "        x = self.classifier(x) # (b, e) -> (b, num_classes)\n",
        "        return x"
      ],
      "metadata": {
        "id": "rhl98a07VU_c"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net(len(tokenizer.key_value), n_layers = 8)\n",
        "model = model.apply(init_weights)"
      ],
      "metadata": {
        "id": "QXFtlOaZXR20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens, mask = x[0], x[1]\n",
        "logits = model(tokens, mask)"
      ],
      "metadata": {
        "id": "7nPnfAxmmUB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape, y.dtype, logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRXAe3YprcdY",
        "outputId": "96d4566d-4832-41b1-c53a-661d44d18673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64]), torch.int64, torch.Size([64, 12687]))"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = AdamW(model.parameters(), lr = max_lr, weight_decay = 1e-02)\n",
        "\n",
        "for i in range(10):\n",
        "    opt.zero_grad()\n",
        "    logits = model(tokens, mask)\n",
        "    loss = loss_fn(logits, y)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMV39Zbtrpxa",
        "outputId": "53912bb7-b06d-4b60-ea2e-e5b4fba08401"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.484833717346191\n",
            "8.209488868713379\n",
            "6.239413738250732\n",
            "4.530508518218994\n",
            "3.043102264404297\n",
            "1.8216582536697388\n",
            "0.9548807144165039\n",
            "0.435703307390213\n",
            "0.1810034066438675\n",
            "0.07764700055122375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtZUL7knrrYJ",
        "outputId": "d43e0d7d-75c0-40bf-ee36-b23d7c83efa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(9.7035, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z99_114Bv7FZ",
        "outputId": "9bf26c1b-d8a3-41f0-ee75-eed30d5fc381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "199"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the initial model"
      ],
      "metadata": {
        "id": "i_KD33-GwJ9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = None\n",
        "del model\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "5qA2klLlwTRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size = 192, shuffle = True)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = Net(len(tokenizer.key_value), n_layers = 6)\n",
        "model = model.apply(init_weights)\n",
        "model = model.to(device)\n",
        "\n",
        "max_lr = 5e-04\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "opt = AdamW(model.parameters(), lr = max_lr, weight_decay = 1e-02)\n",
        "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(opt,\n",
        "                                                   max_lr,\n",
        "                                                   epochs=epochs,\n",
        "                                                   steps_per_epoch=len(dataloader),\n",
        "                                                   pct_start=0.1,\n",
        "                                                   anneal_strategy='cos',\n",
        "                                                   cycle_momentum=True,\n",
        "                                                   base_momentum=0.85,\n",
        "                                                   max_momentum=0.95,\n",
        "                                                   div_factor=100.0,\n",
        "                                                   final_div_factor=1000,\n",
        "                                                   last_epoch=-1)\n",
        "\n",
        "# variances = np.linspace(0.05, 0.15, epochs)\n",
        "train_losses = []\n",
        "for epoch in range(epochs):\n",
        "    train_iter = iter(dataloader)\n",
        "    for it, (x, y) in enumerate(train_iter):\n",
        "\n",
        "        tokens, mask = x[0].to(device), x[1].to(device)\n",
        "        y = y.to(torch.long).to(device)\n",
        "\n",
        "        logits = model(tokens, mask)\n",
        "        loss = loss_fn(logits, y)\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    print(f\"Epoch: {epoch} | Train Loss: {np.mean(train_losses[-len(dataloader):]):.4f}\")\n",
        "    if epoch % 5 == 0: # do checkpointing\n",
        "        # PLS FINISH THE CODE\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': opt.state_dict(),\n",
        "            'scheduler_state_dict': lr_scheduler.state_dict(),\n",
        "            'loss': np.mean(train_losses[-len(dataloader):])\n",
        "        }\n",
        "        torch.save(checkpoint, f'/content/drive/MyDrive/hrd_hack/model_checkpoint_epoch_{epoch}.pt')\n",
        "        print(f\"Checkpoint saved at epoch {epoch}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4HVgSEumWYu",
        "outputId": "4d10aca4-bca0-4cbf-8f80-b0351f2b8294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Train Loss: 10.4448\n",
            "Checkpoint saved at epoch 0\n",
            "Epoch: 1 | Train Loss: 10.2200\n",
            "Epoch: 2 | Train Loss: 10.0366\n",
            "Epoch: 3 | Train Loss: 9.9041\n",
            "Epoch: 4 | Train Loss: 9.8188\n",
            "Epoch: 5 | Train Loss: 9.7992\n",
            "Checkpoint saved at epoch 5\n",
            "Epoch: 6 | Train Loss: 9.7700\n",
            "Epoch: 7 | Train Loss: 9.6116\n",
            "Epoch: 8 | Train Loss: 9.2823\n",
            "Epoch: 9 | Train Loss: 8.8428\n",
            "Epoch: 10 | Train Loss: 8.3100\n",
            "Checkpoint saved at epoch 10\n",
            "Epoch: 11 | Train Loss: 7.7741\n",
            "Epoch: 12 | Train Loss: 7.2526\n",
            "Epoch: 13 | Train Loss: 6.8192\n",
            "Epoch: 14 | Train Loss: 6.3889\n",
            "Epoch: 15 | Train Loss: 6.0309\n",
            "Checkpoint saved at epoch 15\n",
            "Epoch: 16 | Train Loss: 5.6587\n",
            "Epoch: 17 | Train Loss: 5.3335\n",
            "Epoch: 18 | Train Loss: 5.0220\n",
            "Epoch: 19 | Train Loss: 4.7063\n",
            "Epoch: 20 | Train Loss: 4.4632\n",
            "Checkpoint saved at epoch 20\n",
            "Epoch: 21 | Train Loss: 4.1345\n",
            "Epoch: 22 | Train Loss: 3.8826\n",
            "Epoch: 23 | Train Loss: 3.6099\n",
            "Epoch: 24 | Train Loss: 3.3966\n",
            "Epoch: 25 | Train Loss: 3.1438\n",
            "Checkpoint saved at epoch 25\n",
            "Epoch: 26 | Train Loss: 2.9137\n",
            "Epoch: 27 | Train Loss: 2.7332\n",
            "Epoch: 28 | Train Loss: 2.5555\n",
            "Epoch: 29 | Train Loss: 2.3658\n",
            "Epoch: 30 | Train Loss: 2.2922\n",
            "Checkpoint saved at epoch 30\n",
            "Epoch: 31 | Train Loss: 2.0537\n",
            "Epoch: 32 | Train Loss: 1.9238\n",
            "Epoch: 33 | Train Loss: 1.7541\n",
            "Epoch: 34 | Train Loss: 1.6957\n",
            "Epoch: 35 | Train Loss: 1.6450\n",
            "Checkpoint saved at epoch 35\n",
            "Epoch: 36 | Train Loss: 1.5376\n",
            "Epoch: 37 | Train Loss: 1.5124\n",
            "Epoch: 38 | Train Loss: 1.4016\n",
            "Epoch: 39 | Train Loss: 1.3133\n",
            "Epoch: 40 | Train Loss: 1.2822\n",
            "Checkpoint saved at epoch 40\n",
            "Epoch: 41 | Train Loss: 1.2355\n",
            "Epoch: 42 | Train Loss: 1.1831\n",
            "Epoch: 43 | Train Loss: 1.1383\n",
            "Epoch: 44 | Train Loss: 1.0628\n",
            "Epoch: 45 | Train Loss: 1.0330\n",
            "Checkpoint saved at epoch 45\n",
            "Epoch: 46 | Train Loss: 1.0335\n",
            "Epoch: 47 | Train Loss: 1.0328\n",
            "Epoch: 48 | Train Loss: 0.9533\n",
            "Epoch: 49 | Train Loss: 0.9157\n",
            "Epoch: 50 | Train Loss: 0.9038\n",
            "Checkpoint saved at epoch 50\n",
            "Epoch: 51 | Train Loss: 0.8384\n",
            "Epoch: 52 | Train Loss: 0.8440\n",
            "Epoch: 53 | Train Loss: 0.8279\n",
            "Epoch: 54 | Train Loss: 0.7758\n",
            "Epoch: 55 | Train Loss: 0.7840\n",
            "Checkpoint saved at epoch 55\n",
            "Epoch: 56 | Train Loss: 0.7814\n",
            "Epoch: 57 | Train Loss: 0.7425\n",
            "Epoch: 58 | Train Loss: 0.7052\n",
            "Epoch: 59 | Train Loss: 0.7150\n",
            "Epoch: 60 | Train Loss: 0.6839\n",
            "Checkpoint saved at epoch 60\n",
            "Epoch: 61 | Train Loss: 0.6980\n",
            "Epoch: 62 | Train Loss: 0.6757\n",
            "Epoch: 63 | Train Loss: 0.6401\n",
            "Epoch: 64 | Train Loss: 0.6202\n",
            "Epoch: 65 | Train Loss: 0.6228\n",
            "Checkpoint saved at epoch 65\n",
            "Epoch: 66 | Train Loss: 0.5843\n",
            "Epoch: 67 | Train Loss: 0.5750\n",
            "Epoch: 68 | Train Loss: 0.5554\n",
            "Epoch: 69 | Train Loss: 0.5470\n",
            "Epoch: 70 | Train Loss: 0.5675\n",
            "Checkpoint saved at epoch 70\n",
            "Epoch: 71 | Train Loss: 0.5518\n",
            "Epoch: 72 | Train Loss: 0.5407\n",
            "Epoch: 73 | Train Loss: 0.5112\n",
            "Epoch: 74 | Train Loss: 0.5262\n",
            "Epoch: 75 | Train Loss: 0.5140\n",
            "Checkpoint saved at epoch 75\n",
            "Epoch: 76 | Train Loss: 0.4942\n",
            "Epoch: 77 | Train Loss: 0.4728\n",
            "Epoch: 78 | Train Loss: 0.4635\n",
            "Epoch: 79 | Train Loss: 0.4712\n",
            "Epoch: 80 | Train Loss: 0.4451\n",
            "Checkpoint saved at epoch 80\n",
            "Epoch: 81 | Train Loss: 0.4505\n",
            "Epoch: 82 | Train Loss: 0.4266\n",
            "Epoch: 83 | Train Loss: 0.4405\n",
            "Epoch: 84 | Train Loss: 0.4438\n",
            "Epoch: 85 | Train Loss: 0.4252\n",
            "Checkpoint saved at epoch 85\n",
            "Epoch: 86 | Train Loss: 0.4334\n",
            "Epoch: 87 | Train Loss: 0.4183\n",
            "Epoch: 88 | Train Loss: 0.4246\n",
            "Epoch: 89 | Train Loss: 0.4134\n",
            "Epoch: 90 | Train Loss: 0.4140\n",
            "Checkpoint saved at epoch 90\n",
            "Epoch: 91 | Train Loss: 0.4106\n",
            "Epoch: 92 | Train Loss: 0.4125\n",
            "Epoch: 93 | Train Loss: 0.3938\n",
            "Epoch: 94 | Train Loss: 0.4088\n",
            "Epoch: 95 | Train Loss: 0.4120\n",
            "Checkpoint saved at epoch 95\n",
            "Epoch: 96 | Train Loss: 0.3976\n",
            "Epoch: 97 | Train Loss: 0.3959\n",
            "Epoch: 98 | Train Loss: 0.3932\n",
            "Epoch: 99 | Train Loss: 0.3892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': opt.state_dict(),\n",
        "            'scheduler_state_dict': lr_scheduler.state_dict(),\n",
        "            'loss': np.mean(train_losses[-len(dataloader):])\n",
        "        }\n",
        "torch.save(checkpoint, f'/content/drive/MyDrive/hrd_hack/model_checkpoint_epoch_{epoch}.pt')\n",
        "print(f\"Checkpoint saved at epoch {epoch}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jEntpFMouHM",
        "outputId": "5beb12ef-c654-4e83-c10c-c20e8120634a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved at epoch 99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51lCSfylnswb",
        "outputId": "d2d0d6a9-6aa0-40ab-947e-10b8c942885f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((tensor([    0,  1288,  8984, 11900,  9595,  8750,  5646,  5271,  9969, 10099,\n",
              "           7065,  8349, 11069,  3479,  3452,  7142,  6165, 11255,  8534,  7776,\n",
              "           6539,  1662,  7641,  7987,  4184, 11609,  2916,  4622,  9258,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1]),\n",
              "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])),\n",
              " 2)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./drive/MyDrive/hrd_hack/ds_test.pkl', 'wb') as f:\n",
        "    pickle.dump(dataset, f)"
      ],
      "metadata": {
        "id": "EmjcsHCPmXRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.max_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zva4DEZTn8UC",
        "outputId": "d03679ef-7212-421e-bc2f-7d9e2c3bf731"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./drive/MyDrive/hrd_hack/tokenizer_test.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)"
      ],
      "metadata": {
        "id": "fZRABg2Yn5To"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./drive/MyDrive/hrd_hack/ds_test.pkl', 'rb') as f:\n",
        "    dataset_test = pickle.load(f)"
      ],
      "metadata": {
        "id": "HVmK9opwnwc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRkvVLVSnzsi",
        "outputId": "67238989-2340-438a-d565-b17f4bfe1429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((tensor([    0,  7776, 11609,  5271,  6539,  8984,  8647, 10099,  2916,  7142,\n",
              "          11255,  5646,  8750,  1662,  9967,  9969,  1288, 11900,  6165,  8349,\n",
              "           7065,  8534,  3452,  7641,  9595,  9258,  7987,  4184, 11069,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "              1,     1,     1,     1,     1,     1]),\n",
              "  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])),\n",
              " 2)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./drive/MyDrive/hrd_hack/tokenizer_test.pkl', 'rb') as f:\n",
        "    tokenizer_test = pickle.load(f)\n",
        "tokenizer_test.max_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNS3p6kkn1Fo",
        "outputId": "7cffcae9-8ed7-47f0-f908-5c9bad3b65fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lets Create DISEASE Embeddings"
      ],
      "metadata": {
        "id": "VyCKfHFG-UiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1 calculate IC scores for each of the phenotype\n",
        "# for each disease - create a hpo_id_token array and also create a IC score array\n",
        "# Multiply, add and divide by sum of IC Scores\n",
        "# May be Normalize it and visualize"
      ],
      "metadata": {
        "id": "iwfLqgMi-ZuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./drive/MyDrive/hrd_hack/tokenizer_test.pkl', 'rb') as f:\n",
        "    tokenizer_test = pickle.load(f)\n",
        "tokenizer_test.max_len\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zh3VuHcvCOP5",
        "outputId": "d1274054-e420-44ff-9cba-027d0bc6f94a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "256"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./drive/MyDrive/hrd_hack/ds_test.pkl', 'rb') as f:\n",
        "    dataset_test = pickle.load(f)"
      ],
      "metadata": {
        "id": "rgcXX8JfCOWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the frequencies\n",
        "# calculate the -log of frequencies"
      ],
      "metadata": {
        "id": "zmManInnCOf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/disease_to_hpo_with_parents.json', 'r') as f:\n",
        "    disease_to_hpo = json.load(f)"
      ],
      "metadata": {
        "id": "N-i_4sHWCOjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "math.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73Spi6i_NZHN",
        "outputId": "63f7b225-3033-4f6f-ebf5-edf954bbc7b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function math.log>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_diseases = len(disease_to_hpo)\n",
        "symptoms = []\n",
        "for k, v in disease_to_hpo.items():\n",
        "    symptoms += v\n",
        "\n",
        "from collections import Counter\n",
        "num_repetetions = Counter(symptoms)\n",
        "\n",
        "ic_scores = {}\n",
        "for k, v in num_repetetions.items():\n",
        "    ic_scores[k] = -math.log(v / total_diseases)"
      ],
      "metadata": {
        "id": "BaUThJXXMcWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now token_id to ic socre\n",
        "\n",
        "token_id_to_ic = {}\n",
        "for k, v in tokenizer_test.key_value.items():\n",
        "    if k in ic_scores:\n",
        "        token_id_to_ic[v] = ic_scores[k]\n",
        "    else:\n",
        "        token_id_to_ic[v] = 0.0"
      ],
      "metadata": {
        "id": "9R2ALqhKN0K6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dict = torch.load('./drive/MyDrive/hrd_hack/model_checkpoint_epoch_99.pt', map_location = 'cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0BDmg4aPumq",
        "outputId": "893ad13e-03ee-4386-acde-facc8469db11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-2686d48a641f>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_dict = torch.load('./drive/MyDrive/hrd_hack/model_checkpoint_epoch_99.pt', map_location = 'cpu')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_dict.keys()\n",
        "tokenizer = tokenizer_test\n",
        "model = Net(len(tokenizer.key_value), n_layers = 6)\n",
        "# model  = model.apply(init_weights)\n",
        "model.load_state_dict(model_dict['model_state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_zUT83FP3ox",
        "outputId": "0cfc1f1f-40f7-4eb0-d1bc-2f4c37c9d544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embs = model.embedding.weight.detach().cpu()\n",
        "disease = 'OMIM:619340'\n",
        "hpo = disease_to_hpo[disease]\n",
        "hpo_tokens = torch.tensor(tokenizer._tokenize(hpo)).to(torch.long)\n",
        "ic_scores = torch.tensor([token_id_to_ic[int(token)] for token in hpo_tokens])\n",
        "dis_embs = embs[hpo_tokens]\n",
        "\n",
        "weighted_embs = dis_embs * ic_scores.unsqueeze(1)\n",
        "final_disease_emb = weighted_embs.sum(0)\n"
      ],
      "metadata": {
        "id": "KTugJ7TYTK-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hpo_tokens.shape, ic_scores.shape, embs[hpo_tokens].shape, weighted_embs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tM9PhJW0Tx2a",
        "outputId": "7efffc32-cda1-4194-d8cf-d0c265f5ae6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([60]),\n",
              " torch.Size([60]),\n",
              " torch.Size([60, 512]),\n",
              " torch.Size([60, 512]))"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_disease_emb = weighted_embs.sum(0)/ic_scores.sum()\n",
        "final_disease_emb.mean(), final_disease_emb.std()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2q6sJQfUv6R",
        "outputId": "0a3f46f3-558d-46d7-a6d2-07d88a994bd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.0005), tensor(0.0131))"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embs.mean(), embs.std()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QDMmV1tU-_1",
        "outputId": "ae34db59-f760-43c9-9030-f996d7ee1485"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(1.2793e-05), tensor(0.0668))"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "final_disease_embs = []\n",
        "embs = model.embedding.weight.detach().cpu()\n",
        "for disease, did in tqdm(dataset_test.Y_values.items()):\n",
        "    hpo = disease_to_hpo[disease]\n",
        "    hpo_tokens = torch.tensor(tokenizer._tokenize(hpo)).to(torch.long)\n",
        "    ic_scores = torch.tensor([token_id_to_ic[int(token)] for token in hpo_tokens])\n",
        "    dis_embs = embs[hpo_tokens]\n",
        "\n",
        "    weighted_embs = dis_embs * ic_scores.unsqueeze(1)\n",
        "    final_disease_emb = weighted_embs.sum(0)\n",
        "    final_disease_embs.append(final_disease_emb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfmLrCfIRih3",
        "outputId": "c6c6f29c-c6e5-48ce-d19b-2b56b66ace87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12687/12687 [00:07<00:00, 1709.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "disease_embs = torch.stack(final_disease_embs)\n",
        "disease_embs = (disease_embs - disease_embs.mean(0))/disease_embs.std(0)"
      ],
      "metadata": {
        "id": "Kx1DAPekXJq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disease_embs.mean(), disease_embs.std(), disease_embs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xk1rzEOKXX8o",
        "outputId": "38d9bcaa-b289-4a31-acf2-90e245f94b4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(2.6867e-11), tensor(1.0000), torch.Size([12687, 512]))"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(disease_embs, './drive/MyDrive/hrd_hack/disease_embs.pt')"
      ],
      "metadata": {
        "id": "uYIt9ADVX92w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def l2_distance(emb1, emb2):\n",
        "    assert emb1.shape == emb2.shape\n",
        "    diff_squared = (emb1 - emb2)**2\n",
        "    return torch.sqrt(diff_squared.sum())\n",
        "\n",
        "print(torch.dist(disease_embs[0], disease_embs[1], p = 2))\n",
        "print(l2_distance(disease_embs[0], disease_embs[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jx29NDdXYSAm",
        "outputId": "707d908d-2967-44d8-9ec5-fc6bd27a799c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(18.4131)\n",
            "tensor(18.4131)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate inter embedding distances and then plot a histogram\n",
        "distances = []\n",
        "def l2_distance(emb1, emb2):\n",
        "    assert emb1.shape == emb2.shape\n",
        "    diff_squared = (emb1 - emb2)**2\n",
        "    return torch.sqrt(diff_squared.sum())\n",
        "\n",
        "for i in tqdm(range(disease_embs.shape[0])):\n",
        "    for j in range(i+1, disease_embs.shape[0]):\n",
        "        distances.append(torch.dist(disease_embs[i], disease_embs[j], p = 2).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmpHfVsVYGwn",
        "outputId": "00c57d2b-69df-4238-c3dc-ed6ebb4b96a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12687/12687 [22:45<00:00,  9.29it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(distances)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-1FUTYwgtxP",
        "outputId": "844ff69e-0200-4e47-e13d-4bb9b42f2447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80473641"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.mean(distances), np.std(distances), min(distances), max(distances)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06gueh-vYG9Q",
        "outputId": "15a2162a-5370-4eff-edbf-8433b16efd25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30.073520401521733"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('./drive/MyDrive/hrd_hack/distances.pkl', 'wb') as f:\n",
        "    pickle.dump(distances, f)"
      ],
      "metadata": {
        "id": "Gt-5v_kmhH0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./drive/MyDrive/hrd_hack/distances.pkl', 'rb') as f:\n",
        "    distances = pickle.load(f)"
      ],
      "metadata": {
        "id": "9Eje7S8qhocW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distances = np.array(distances)"
      ],
      "metadata": {
        "id": "cdownwimiX7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.pyplot import hist\n",
        "hist(distances, bins = 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xiexk-f2g1FX",
        "outputId": "e5330a95-f82c-4d9c-92f6-6117737217f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([2.792000e+03, 7.246000e+03, 2.016900e+04, 5.180600e+04,\n",
              "        1.209060e+05, 2.378580e+05, 3.817160e+05, 5.778580e+05,\n",
              "        8.187070e+05, 1.105850e+06, 1.424407e+06, 1.791895e+06,\n",
              "        2.149236e+06, 2.483662e+06, 2.861260e+06, 3.205447e+06,\n",
              "        3.490353e+06, 3.721430e+06, 3.899931e+06, 3.994004e+06,\n",
              "        3.998273e+06, 3.971849e+06, 3.875165e+06, 3.745792e+06,\n",
              "        3.540908e+06, 3.325943e+06, 3.090640e+06, 2.816880e+06,\n",
              "        2.580256e+06, 2.349676e+06, 2.111918e+06, 1.891955e+06,\n",
              "        1.669309e+06, 1.456938e+06, 1.270733e+06, 1.089673e+06,\n",
              "        9.306780e+05, 7.850970e+05, 6.473860e+05, 5.484830e+05,\n",
              "        4.672380e+05, 3.917680e+05, 3.178940e+05, 2.508240e+05,\n",
              "        2.017340e+05, 1.684290e+05, 1.327610e+05, 1.022770e+05,\n",
              "        8.364100e+04, 6.756500e+04, 5.465000e+04, 4.295800e+04,\n",
              "        3.340300e+04, 2.766200e+04, 2.166100e+04, 1.533900e+04,\n",
              "        1.097700e+04, 7.803000e+03, 5.377000e+03, 3.882000e+03,\n",
              "        2.768000e+03, 1.885000e+03, 1.363000e+03, 9.450000e+02,\n",
              "        7.060000e+02, 4.700000e+02, 3.150000e+02, 2.390000e+02,\n",
              "        1.930000e+02, 1.810000e+02, 1.290000e+02, 1.060000e+02,\n",
              "        1.560000e+02, 2.220000e+02, 2.870000e+02, 4.860000e+02,\n",
              "        7.430000e+02, 1.447000e+03, 2.316000e+03, 1.816000e+03,\n",
              "        1.411000e+03, 1.086000e+03, 7.750000e+02, 5.670000e+02,\n",
              "        3.770000e+02, 2.290000e+02, 1.880000e+02, 1.240000e+02,\n",
              "        6.500000e+01, 4.200000e+01, 3.000000e+01, 1.400000e+01,\n",
              "        1.300000e+01, 5.000000e+00, 6.000000e+00, 1.000000e+00,\n",
              "        1.000000e+00, 4.000000e+00, 0.000000e+00, 2.000000e+00]),\n",
              " array([  0.        ,   1.32224747,   2.64449493,   3.9667424 ,\n",
              "          5.28898987,   6.61123734,   7.9334848 ,   9.25573227,\n",
              "         10.57797974,  11.9002272 ,  13.22247467,  14.54472214,\n",
              "         15.8669696 ,  17.18921707,  18.51146454,  19.83371201,\n",
              "         21.15595947,  22.47820694,  23.80045441,  25.12270187,\n",
              "         26.44494934,  27.76719681,  29.08944427,  30.41169174,\n",
              "         31.73393921,  33.05618668,  34.37843414,  35.70068161,\n",
              "         37.02292908,  38.34517654,  39.66742401,  40.98967148,\n",
              "         42.31191895,  43.63416641,  44.95641388,  46.27866135,\n",
              "         47.60090881,  48.92315628,  50.24540375,  51.56765121,\n",
              "         52.88989868,  54.21214615,  55.53439362,  56.85664108,\n",
              "         58.17888855,  59.50113602,  60.82338348,  62.14563095,\n",
              "         63.46787842,  64.79012589,  66.11237335,  67.43462082,\n",
              "         68.75686829,  70.07911575,  71.40136322,  72.72361069,\n",
              "         74.04585815,  75.36810562,  76.69035309,  78.01260056,\n",
              "         79.33484802,  80.65709549,  81.97934296,  83.30159042,\n",
              "         84.62383789,  85.94608536,  87.26833282,  88.59058029,\n",
              "         89.91282776,  91.23507523,  92.55732269,  93.87957016,\n",
              "         95.20181763,  96.52406509,  97.84631256,  99.16856003,\n",
              "        100.4908075 , 101.81305496, 103.13530243, 104.4575499 ,\n",
              "        105.77979736, 107.10204483, 108.4242923 , 109.74653976,\n",
              "        111.06878723, 112.3910347 , 113.71328217, 115.03552963,\n",
              "        116.3577771 , 117.68002457, 119.00227203, 120.3245195 ,\n",
              "        121.64676697, 122.96901443, 124.2912619 , 125.61350937,\n",
              "        126.93575684, 128.2580043 , 129.58025177, 130.90249924,\n",
              "        132.2247467 ]),\n",
              " <BarContainer object of 100 artists>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(distances)"
      ],
      "metadata": {
        "id": "yUObhpcjion7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write a reward function"
      ],
      "metadata": {
        "id": "rVc-JYpR-agK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('/content/disease_to_hpo_with_parents.json', 'r') as f:\n",
        "    data = json.load(f)"
      ],
      "metadata": {
        "id": "FlUrOqIhA7NH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lens = []\n",
        "for k, v in data.items():\n",
        "    lens.append(len(v))\n",
        "np.mean(lens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RU5fBEZlA8fM",
        "outputId": "9601fa47-c140-4250-ba45-25aa13295e27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21.369196815638055"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(lens, bins = 100)"
      ],
      "metadata": {
        "id": "ONzB-to8A8Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(lens)//2\n",
        "sorted(lens)[len(lens)//2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDXJ4XmjA8W9",
        "outputId": "1a000fbb-9ef6-4b0d-cf1c-d5c983201c10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RewardFn():\n",
        "    '''\n",
        "     -alpha(t) * l2_distance\n",
        "\n",
        "    where alpha(t) = α_base + α_scale × (t / t_max)\n",
        "    '''\n",
        "    def __init__(self, max_steps = 22, threshold = 0.5, base = 1.):\n",
        "        self.max_steps = max_steps\n",
        "        self.alpha_base = base\n",
        "        self.alpha_scale = 0.05\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def calculate_reward(self, emb_hat, emb, t):\n",
        "        distances = torch.dist(emb_hat, emb, p = 2)\n",
        "\n",
        "        done = distances < self.threshold\n",
        "        alpha = torch.ones_like(distances, device = emb_hat.device) * self.alpha_base\n",
        "        emb = emb.to(emb_hat.device)\n",
        "\n",
        "        not_done_mask = ~done\n",
        "        done_mask = done\n",
        "\n",
        "        alpha[not_done_mask] += self.alpha_scale * (t / self.max_steps)\n",
        "\n",
        "        # alpha[done_mask] += 1.0\n",
        "        reward = -alpha * distances\n",
        "\n",
        "        return reward, distances"
      ],
      "metadata": {
        "id": "Pa7pm2aRDRKr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps = 22\n",
        "alpha_base = 0.01\n",
        "alpha_scale = 1.\n",
        "\n",
        "phs = []\n",
        "for t in range(max_steps):\n",
        "    alpha = alpha_base + alpha_scale * (t / max_steps)\n",
        "    phs.append(alpha)\n",
        "\n",
        "plt.plot(phs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "4q-_0s9iDRIA",
        "outputId": "cfe5915a-74cd-4709-c850-8292bd230544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fd5a7c92b10>]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement a POC on 10 to 20 Diseases"
      ],
      "metadata": {
        "id": "-O1TGZwd-q-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pickle\n",
        "import random\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "from einops import rearrange\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from collections import namedtuple"
      ],
      "metadata": {
        "id": "SMclvOaCvecK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets first create a dataset\n",
        "# Actor Network and Critic Network - Symptom and Disease\n",
        "\n",
        "class POCDS(Dataset):\n",
        "    def __init__(self, data_dict, tokenizer, Y_values, Y_embeddings):\n",
        "        self.data = data_dict\n",
        "\n",
        "        self.X = list(data_dict.values())\n",
        "        self.Y = list(data_dict.keys())\n",
        "        self.Y_values = Y_values\n",
        "        self.Y_embeddings = Y_embeddings\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        x = self.X[idx]\n",
        "        y = self.Y[idx]\n",
        "\n",
        "        random.shuffle(x)\n",
        "\n",
        "        if len(x) >= self.tokenizer.max_len - 1:\n",
        "            x = x[:self.tokenizer.max_len - 1]\n",
        "\n",
        "        y_val = self.Y_values[y]\n",
        "        y_emb = self.Y_embeddings[y_val]\n",
        "\n",
        "        tokens, mask = self.tokenizer.tokenize(x)\n",
        "        return (tokens, mask), y_emb"
      ],
      "metadata": {
        "id": "kQzRRtbyed19"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/hrd_hack/ds_test.pkl', 'rb') as f:\n",
        "    original_ds = pickle.load(f)\n",
        "\n",
        "disease_embs = torch.load('/content/drive/MyDrive/hrd_hack/disease_embs.pt')\n",
        "disease_embs = disease_embs.detach().cpu()\n",
        "\n",
        "with open('/content/drive/MyDrive/hrd_hack/disease_to_hpo_with_parents.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "new_data = {}\n",
        "keys = list(data.keys())\n",
        "for i in range(100):\n",
        "    new_data[keys[i]] = data[keys[i]]\n",
        "\n",
        "tokenizer = original_ds.tokenizer\n",
        "dataset = POCDS(new_data, tokenizer, original_ds.Y_values, disease_embs)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size = 100, shuffle = True)\n",
        "x, y = next(iter(dataloader))\n",
        "\n",
        "x[0].shape, x[1].shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqSnjGk7xJZH",
        "outputId": "8b08c563-3ae4-4f56-f0c6-0e161178e692"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-19acedff0cb1>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  disease_embs = torch.load('/content/drive/MyDrive/hrd_hack/disease_embs.pt')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([100, 256]), torch.Size([100, 256]), torch.Size([100, 512]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class POCActor(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim = 512, n_classes = 12687, n_layers = 8, n_heads = 16):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.norm = nn.LayerNorm(emb_dim)\n",
        "        self.transformers = nn.ModuleList([EncoderLayer(d_model = emb_dim, nhead = n_heads) for _ in range(n_layers)])\n",
        "\n",
        "        self.final_norm = nn.LayerNorm(emb_dim)\n",
        "\n",
        "        self.emb_transform_mean = nn.Sequential(\n",
        "            nn.Linear(emb_dim, 1024),\n",
        "            nn.GELU(approximate = 'tanh'),\n",
        "            nn.Linear(1024, 512)\n",
        "        )\n",
        "        self.emb_transform_log_std = nn.Sequential(\n",
        "            nn.Linear(emb_dim, 1024),\n",
        "            nn.GELU(approximate = 'tanh'),\n",
        "            nn.Linear(1024, 512)\n",
        "        )\n",
        "\n",
        "    def load_backbone(self, path = './drive/MyDrive/hrd_hack/model_checkpoint_epoch_99.pt'):\n",
        "        checkpoint = torch.load(path, map_location = 'cpu')['model_state_dict']\n",
        "        self_state_dict = self.state_dict()\n",
        "        for k, v in checkpoint.items():\n",
        "            if k in self_state_dict:\n",
        "                if v.shape == self_state_dict[k].shape:\n",
        "                    self_state_dict[k].copy_(v)\n",
        "\n",
        "    def adjust_log_std(self, log_std):\n",
        "        log_std_min, log_std_max = (-5, 2)  # From SpinUp / Denis Yarats\n",
        "        return log_std_min + 0.5 * (log_std_max - log_std_min) * (log_std + 1)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.embedding(x) # (b, s, e)\n",
        "        x = self.norm(x) # (b, s, e)\n",
        "        mask = mask.bool()\n",
        "        for transformer in self.transformers:\n",
        "            x = transformer(x, mask)\n",
        "\n",
        "        x = x[:, 0, :].squeeze() # (b, s, e) -> (b, e)\n",
        "\n",
        "        x = self.final_norm(x).squeeze() # (b, e)\n",
        "\n",
        "        mean = self.emb_transform_mean(x) # (b, e) -> (b, e)\n",
        "\n",
        "        log_std = self.emb_transform_log_std(x) # (b, e) -> (b, e)\n",
        "        log_std = torch.tanh(log_std)\n",
        "        return mean, self.adjust_log_std(log_std)\n",
        "\n",
        "    def get_action(self, x, mask):\n",
        "        mean, log_std = self.forward(x, mask)\n",
        "        std = torch.exp(log_std)\n",
        "        normal = torch.distributions.Normal(mean, std)\n",
        "        x_t = normal.rsample()\n",
        "        log_prob = normal.log_prob(x_t)\n",
        "        return x_t, log_prob\n",
        "\n",
        "class POCCritic(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim = 512, n_classes = 12687, n_layers = 8, n_heads = 16):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.disease_transform = nn.Sequential(\n",
        "            nn.Linear(emb_dim, 1024),\n",
        "            nn.GELU(approximate = 'tanh'),\n",
        "            nn.Linear(1024, 512)\n",
        "        )\n",
        "\n",
        "        self.norm = nn.LayerNorm(emb_dim)\n",
        "        self.transformers = nn.ModuleList([EncoderLayer(d_model = emb_dim, nhead = n_heads) for _ in range(n_layers)])\n",
        "        self.final_norm = nn.LayerNorm(emb_dim)\n",
        "        self.q_value = nn.Sequential(\n",
        "            nn.Linear(emb_dim, 1024),\n",
        "            nn.GELU(approximate = 'tanh'),\n",
        "            nn.Linear(1024, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask, d_emb):\n",
        "        # breakpoint()\n",
        "\n",
        "        x = self.embedding(x) # (b, s, e)\n",
        "\n",
        "        d_emb = self.disease_transform(d_emb) # (b, e)\n",
        "        d_emb = d_emb.unsqueeze(1) # (b, s + 1, e)\n",
        "        d_emb_mask = torch.ones(x.shape[0], 1)\n",
        "\n",
        "        mask = torch.cat([mask, d_emb_mask], dim = -1)# (b, s+1, 1)\n",
        "        x = torch.cat([x, d_emb], dim = 1)\n",
        "\n",
        "        x = self.norm(x) # (b, s+1, e)\n",
        "        mask = mask.bool()\n",
        "        for transformer in self.transformers:\n",
        "            x = transformer(x, mask)\n",
        "\n",
        "        x = x[:, 0, :].squeeze() # (b, s, e) -> (b, e)\n",
        "        x = self.final_norm(x)\n",
        "        return self.q_value(x)"
      ],
      "metadata": {
        "id": "GHMCW87wygAW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actor = POCActor(len(tokenizer.key_value), n_layers = 6)\n",
        "critic = POCCritic(len(tokenizer.key_value), n_layers = 2)\n",
        "actor.load_backbone()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65I5QP-qyf90",
        "outputId": "18c6d60a-167b-4eba-ea29-91b10ee6edf0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-3db0f11a7521>:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(path, map_location = 'cpu')['model_state_dict']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "class Batch:\n",
        "\n",
        "    '''\n",
        "    stores T - current time step\n",
        "    State - Current X\n",
        "    All X - Remaining X\n",
        "    '''\n",
        "    def __init__(self, phenos: list[list], embs: torch.Tensor,  tokenizer: Tokenizer, max_t = 50):\n",
        "        import copy\n",
        "\n",
        "        self.X = copy.deepcopy(phenos)\n",
        "        self.Y = embs\n",
        "\n",
        "        self.state_x = None\n",
        "        self.max_t = max_t\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def get_sample(self):\n",
        "\n",
        "        for phenotypes, present_state in zip(self.X, self.state_x):\n",
        "            if len(phenotypes) > 0:\n",
        "                random.shuffle(phenotypes)\n",
        "                present_state.append(phenotypes.pop(0))\n",
        "\n",
        "        # breakpoint()\n",
        "        tokenized_state_x = []\n",
        "        tokenized_state_mask = []\n",
        "\n",
        "        for state in self.state_x:\n",
        "            state_tokenized, state_mask = self.tokenizer.tokenize(state)\n",
        "            tokenized_state_x.append(state_tokenized)\n",
        "            tokenized_state_mask.append(state_mask)\n",
        "\n",
        "        tokenized_state_x = torch.stack(tokenized_state_x)\n",
        "        tokenized_state_mask = torch.stack(tokenized_state_mask)\n",
        "        return (self.t, tokenized_state_x, tokenized_state_mask), self.Y\n",
        "\n",
        "    def get_next(self):\n",
        "\n",
        "        if hasattr(self, 't'):\n",
        "            self.t += 1\n",
        "        else:\n",
        "            self.t = 0\n",
        "            self.state_x = [[] for _ in range(len(self.X))]\n",
        "\n",
        "        eps_done = self.t > self.max_t\n",
        "        (current_t, tokenized_state_x, tokenized_state_mask), y = self.get_sample()\n",
        "\n",
        "        return_obj = namedtuple('StateBatch', ['eps_done', 'current_t', 'tokenized_state_x', 'tokenized_state_mask', 'y'])\n",
        "        return return_obj(eps_done, current_t, tokenized_state_x, tokenized_state_mask, y)\n"
      ],
      "metadata": {
        "id": "54Nh5mPHT_bA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/hrd_hack/disease_to_hpo_with_parents.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/hrd_hack/ds_test.pkl', 'rb') as f:\n",
        "    original_ds = pickle.load(f)\n",
        "\n",
        "disease_embs = torch.load('/content/drive/MyDrive/hrd_hack/disease_embs.pt')\n",
        "disease_embs = disease_embs.detach().cpu()\n",
        "\n",
        "new_data = {}\n",
        "keys = list(data.keys())\n",
        "for i in range(100):\n",
        "    new_data[keys[i]] = data[keys[i]]\n",
        "\n",
        "poc_phenotypes = list(new_data.values())\n",
        "\n",
        "y_values = original_ds.Y_values\n",
        "diseases = list(new_data.keys())\n",
        "disease_ids = [y_values[disease] for disease in diseases]\n",
        "poc_disease_embs = disease_embs[disease_ids]\n",
        "\n",
        "tokenizer = original_ds.tokenizer\n",
        "replay_buffer = Batch(poc_phenotypes, poc_disease_embs, tokenizer)"
      ],
      "metadata": {
        "id": "hFn96buIT_Y_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea8d0988-b87a-46bb-bcf1-4b08ed8b0b0a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-ee52db2fd361>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  disease_embs = torch.load('/content/drive/MyDrive/hrd_hack/disease_embs.pt')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_obj = replay_buffer.get_next()\n",
        "tokenized_state_x = state_obj.tokenized_state_x[:4]\n",
        "tokenized_state_mask = state_obj.tokenized_state_mask[:4]\n",
        "y = state_obj.y[:4]"
      ],
      "metadata": {
        "id": "CVii74dslJPL"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_obj.current_t, tokenized_state_x.shape, tokenized_state_mask.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jttBgR8KFbVq",
        "outputId": "3d743325-a440-4e26-b868-ed26ef60c5c9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, torch.Size([4, 256]), torch.Size([4, 256]), torch.Size([4, 512]))"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test one loop\n",
        "action, log_prob = actor.get_action(tokenized_state_x, tokenized_state_mask)\n",
        "q_value = critic(tokenized_state_x, tokenized_state_mask, y)\n",
        "\n",
        "print(action.shape, log_prob.shape, q_value.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFjR8LiOQYz0",
        "outputId": "d4225dae-3c8c-48bd-91c9-1b6567b1140c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 512]) torch.Size([4, 512]) torch.Size([4, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reward_fn = RewardFn(max_steps = 22, threshold = 0.5)\n",
        "reward, _ = reward_fn.calculate_reward(action, y, 4)\n",
        "reward"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZaSvlr1WoqR",
        "outputId": "9bcccd61-95c2-45bd-b687-0cefa38f817f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-0.7727, grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch_obj(size = 32):\n",
        "    with open('/content/drive/MyDrive/hrd_hack/disease_to_hpo_with_parents.json', 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    with open('/content/drive/MyDrive/hrd_hack/ds_test.pkl', 'rb') as f:\n",
        "        original_ds = pickle.load(f)\n",
        "\n",
        "    disease_embs = torch.load('/content/drive/MyDrive/hrd_hack/disease_embs.pt')\n",
        "    disease_embs = disease_embs.detach().cpu()\n",
        "\n",
        "    new_data = {}\n",
        "    keys = list(data.keys())\n",
        "    for i in range(size):\n",
        "        new_data[keys[i]] = data[keys[i]]\n",
        "\n",
        "    poc_phenotypes = list(new_data.values())\n",
        "\n",
        "    y_values = original_ds.Y_values\n",
        "    diseases = list(new_data.keys())\n",
        "    disease_ids = [y_values[disease] for disease in diseases]\n",
        "    poc_disease_embs = disease_embs[disease_ids]\n",
        "\n",
        "    tokenizer = original_ds.tokenizer\n",
        "    replay_buffer = Batch(poc_phenotypes, poc_disease_embs, tokenizer)\n",
        "    return replay_buffer"
      ],
      "metadata": {
        "id": "R4QnHRSkYN5R"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write the training loop directly - As its a POC, the objective is to see the model overfits or not\n",
        "with open('/content/drive/MyDrive/hrd_hack/ds_test.pkl', 'rb') as f:\n",
        "    original_ds = pickle.load(f)\n",
        "\n",
        "tokenizer = original_ds.tokenizer\n",
        "\n",
        "max_eps_len = 20\n",
        "gamma = 0.9\n",
        "alpha = 0.2\n",
        "epochs = 100\n",
        "\n",
        "actor = POCActor(len(tokenizer.key_value), n_layers = 6)\n",
        "critic_1 = POCCritic(len(tokenizer.key_value), n_layers = 2)\n",
        "critic_2 = POCCritic(len(tokenizer.key_value), n_layers = 2)\n",
        "actor.load_backbone()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "actor.to(device)\n",
        "critic_1.to(device)\n",
        "critic_2.to(device)\n",
        "\n",
        "reward_fn = RewardFn(max_steps = 22, threshold = 0.5, base = 2.)\n",
        "\n",
        "max_lr_actor = 1e-04\n",
        "max_lr_critic = 1e-03\n",
        "\n",
        "critics_params = list(critic_1.parameters()) + list(critic_2.parameters())\n",
        "actor_optimizer = AdamW(actor.parameters(), lr = max_lr_actor)\n",
        "critics_optimizer = AdamW(critics_params, lr = max_lr_critic)\n",
        "\n",
        "lr_scheduler_actor = OneCycleLR(actor_optimizer, max_lr = max_lr_actor, total_steps = 100)\n",
        "lr_scheduler_critics = OneCycleLR(critics_optimizer, max_lr = max_lr_critic, total_steps = 100)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    batch = get_batch_obj(8)\n",
        "    state = batch.get_next()\n",
        "    for eps in range(max_eps_len):\n",
        "        with torch.no_grad():\n",
        "            action, log_prob = actor.get_action(state.tokenized_state_x, state.tokenized_state_mask)\n",
        "            reward, distance = reward_fn.calculate_reward(action, state.y, state.current_t)\n",
        "\n",
        "            next_state = batch.get_next()\n",
        "            next_action, distance_next = actor.get_action(next_state.tokenized_state_x, next_state.tokenized_state_mask)\n",
        "\n",
        "            target_q1 = reward + gamma*critic_1(next_state.tokenized_state_x, next_state.tokenized_state_mask, next_action)\n",
        "            target_q2 = reward + gamma*critic_2(next_state.tokenized_state_x, next_state.tokenized_state_mask, next_action)\n",
        "\n",
        "        q_value_1 = critic_1(state.tokenized_state_x, state.tokenized_state_mask, action)\n",
        "        q_value_2 = critic_2(state.tokenized_state_x, state.tokenized_state_mask, action)\n",
        "        # bellmans eqn: Q(s, a) = reward + gamma * Q(s_next, a_next)\n",
        "        # LHS == RHS, Loss FN = MSE(LHS< RHS)\n",
        "        critic_loss = F.mse_loss(q_value_1, target_q1) + F.mse_loss(q_value_2, target_q2)\n",
        "        critics_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        critics_optimizer.step()\n",
        "        # lr_scheduler_critics.step()\n",
        "\n",
        "        if eps % 5 == 0:\n",
        "\n",
        "            action, log_prob = actor.get_action(state.tokenized_state_x, state.tokenized_state_mask)\n",
        "            q_value_1 = critic_1(state.tokenized_state_x, state.tokenized_state_mask, action)\n",
        "            q_value_2 = critic_2(state.tokenized_state_x, state.tokenized_state_mask, action)\n",
        "\n",
        "            target_q = torch.min(q_value_1, q_value_2) #bs, 1\n",
        "            actor_objective = (target_q - alpha*log_prob.sum(1)).mean() # bs, 512 -> bs, 1 -> 1\n",
        "            actor_loss = -actor_objective\n",
        "            actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            actor_optimizer.step()\n",
        "            # lr_scheduler_actor.step()\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            print(f'epoch: {epoch}, episode: {eps}, Reward: {reward.item():.4f}, distance: {distance.item():.4f}, Critic Loss: {critic_loss.detach().cpu().item():.4f}, actor_q_value: {actor_objective.detach().cpu().item():.4f}')\n"
      ],
      "metadata": {
        "id": "ehRG6weQT_Tz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df29a4f-c70a-4885-e9fa-ecaa63995775"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-3db0f11a7521>:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(path, map_location = 'cpu')['model_state_dict']\n",
            "<ipython-input-11-75dfc8ab8891>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  disease_embs = torch.load('/content/drive/MyDrive/hrd_hack/disease_embs.pt')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, episode: 0, Reward: -108.0592, distance: 54.0296, Critic Loss: 23350.2852, actor_q_value: -9.4810\n",
            "epoch: 0, episode: 5, Reward: -108.7010, distance: 54.2888, Critic Loss: 23495.4160, actor_q_value: -8.3181\n",
            "epoch: 0, episode: 10, Reward: -107.6353, distance: 53.4532, Critic Loss: 22996.0312, actor_q_value: -13.3044\n",
            "epoch: 0, episode: 15, Reward: -108.1517, distance: 53.4083, Critic Loss: 23168.2031, actor_q_value: -15.7872\n",
            "epoch: 1, episode: 0, Reward: -105.8029, distance: 52.9014, Critic Loss: 22148.1797, actor_q_value: -13.6682\n",
            "epoch: 1, episode: 5, Reward: -108.1269, distance: 54.0021, Critic Loss: 23109.8242, actor_q_value: -15.6342\n",
            "epoch: 1, episode: 10, Reward: -106.9527, distance: 53.1142, Critic Loss: 22543.4258, actor_q_value: -13.6578\n",
            "epoch: 1, episode: 15, Reward: -108.1048, distance: 53.3851, Critic Loss: 22991.8945, actor_q_value: -16.4860\n",
            "epoch: 2, episode: 0, Reward: -108.6328, distance: 54.3164, Critic Loss: 23195.8594, actor_q_value: -13.8342\n",
            "epoch: 2, episode: 5, Reward: -108.3831, distance: 54.1300, Critic Loss: 23050.7461, actor_q_value: -15.6061\n",
            "epoch: 2, episode: 10, Reward: -108.6748, distance: 53.9694, Critic Loss: 23110.8438, actor_q_value: -17.2675\n",
            "epoch: 2, episode: 15, Reward: -107.0983, distance: 52.8881, Critic Loss: 22390.0391, actor_q_value: -19.1153\n",
            "epoch: 3, episode: 0, Reward: -108.2857, distance: 54.1429, Critic Loss: 22855.4336, actor_q_value: -17.2360\n",
            "epoch: 3, episode: 5, Reward: -107.6292, distance: 53.7535, Critic Loss: 22527.1094, actor_q_value: -20.8006\n",
            "epoch: 3, episode: 10, Reward: -108.2472, distance: 53.7571, Critic Loss: 22733.4219, actor_q_value: -22.6337\n",
            "epoch: 3, episode: 15, Reward: -109.6847, distance: 54.1653, Critic Loss: 23297.8750, actor_q_value: -22.0030\n",
            "epoch: 4, episode: 0, Reward: -108.5436, distance: 54.2718, Critic Loss: 22757.4805, actor_q_value: -21.4800\n",
            "epoch: 4, episode: 5, Reward: -109.0943, distance: 54.4852, Critic Loss: 22939.2734, actor_q_value: -21.9975\n",
            "epoch: 4, episode: 10, Reward: -108.5924, distance: 53.9285, Critic Loss: 22661.1406, actor_q_value: -24.6264\n",
            "epoch: 4, episode: 15, Reward: -107.2511, distance: 52.9635, Critic Loss: 22034.8203, actor_q_value: -27.7119\n",
            "epoch: 5, episode: 0, Reward: -107.2913, distance: 53.6457, Critic Loss: 21995.6758, actor_q_value: -24.6818\n",
            "epoch: 5, episode: 5, Reward: -108.2923, distance: 54.0847, Critic Loss: 22356.3223, actor_q_value: -29.7584\n",
            "epoch: 5, episode: 10, Reward: -110.6366, distance: 54.9437, Critic Loss: 23289.6523, actor_q_value: -30.1490\n",
            "epoch: 5, episode: 15, Reward: -110.5407, distance: 54.5880, Critic Loss: 23181.3750, actor_q_value: -28.8296\n",
            "epoch: 6, episode: 0, Reward: -107.8792, distance: 53.9396, Critic Loss: 21985.9805, actor_q_value: -30.9109\n",
            "epoch: 6, episode: 5, Reward: -106.8676, distance: 53.3731, Critic Loss: 21496.3965, actor_q_value: -32.9237\n",
            "epoch: 6, episode: 10, Reward: -108.8036, distance: 54.0334, Critic Loss: 22233.8164, actor_q_value: -33.8859\n",
            "epoch: 6, episode: 15, Reward: -110.8929, distance: 54.7619, Critic Loss: 23050.2324, actor_q_value: -36.5083\n",
            "epoch: 7, episode: 0, Reward: -108.5055, distance: 54.2527, Critic Loss: 21965.6367, actor_q_value: -35.8758\n",
            "epoch: 7, episode: 5, Reward: -110.5625, distance: 55.2185, Critic Loss: 22759.9043, actor_q_value: -38.0881\n",
            "epoch: 7, episode: 10, Reward: -112.3710, distance: 55.8050, Critic Loss: 23455.9277, actor_q_value: -40.4071\n",
            "epoch: 7, episode: 15, Reward: -109.9071, distance: 54.2751, Critic Loss: 22321.5078, actor_q_value: -42.8468\n",
            "epoch: 8, episode: 0, Reward: -109.1306, distance: 54.5653, Critic Loss: 21915.7695, actor_q_value: -41.8524\n",
            "epoch: 8, episode: 5, Reward: -109.9302, distance: 54.9027, Critic Loss: 22167.2969, actor_q_value: -44.6347\n",
            "epoch: 8, episode: 10, Reward: -109.8711, distance: 54.5635, Critic Loss: 22055.6953, actor_q_value: -45.3902\n",
            "epoch: 8, episode: 15, Reward: -110.8213, distance: 54.7266, Critic Loss: 22369.3828, actor_q_value: -51.4965\n",
            "epoch: 9, episode: 0, Reward: -111.1095, distance: 55.5547, Critic Loss: 22403.2695, actor_q_value: -48.2894\n",
            "epoch: 9, episode: 5, Reward: -109.0607, distance: 54.4685, Critic Loss: 21452.7695, actor_q_value: -51.5719\n",
            "epoch: 9, episode: 10, Reward: -111.3625, distance: 55.3042, Critic Loss: 22322.3906, actor_q_value: -54.2558\n",
            "epoch: 9, episode: 15, Reward: -112.3718, distance: 55.4923, Critic Loss: 22654.8242, actor_q_value: -57.0643\n",
            "epoch: 10, episode: 0, Reward: -109.1022, distance: 54.5511, Critic Loss: 21189.6523, actor_q_value: -55.7081\n",
            "epoch: 10, episode: 5, Reward: -110.8401, distance: 55.3572, Critic Loss: 21810.7188, actor_q_value: -60.9787\n",
            "epoch: 10, episode: 10, Reward: -110.1465, distance: 54.7003, Critic Loss: 21421.4785, actor_q_value: -65.3484\n",
            "epoch: 10, episode: 15, Reward: -111.7120, distance: 55.1664, Critic Loss: 21971.5508, actor_q_value: -65.8164\n",
            "epoch: 11, episode: 0, Reward: -111.6373, distance: 55.8187, Critic Loss: 21835.4277, actor_q_value: -63.0898\n",
            "epoch: 11, episode: 5, Reward: -113.1178, distance: 56.4947, Critic Loss: 22348.4180, actor_q_value: -67.4499\n",
            "epoch: 11, episode: 10, Reward: -110.5368, distance: 54.8941, Critic Loss: 21162.1953, actor_q_value: -74.7192\n",
            "epoch: 11, episode: 15, Reward: -112.3949, distance: 55.5036, Critic Loss: 21822.4102, actor_q_value: -76.5437\n",
            "epoch: 12, episode: 0, Reward: -111.3249, distance: 55.6624, Critic Loss: 21265.2852, actor_q_value: -73.1892\n",
            "epoch: 12, episode: 5, Reward: -112.8405, distance: 56.3562, Critic Loss: 21778.2422, actor_q_value: -76.1841\n",
            "epoch: 12, episode: 10, Reward: -112.0727, distance: 55.6569, Critic Loss: 21342.1211, actor_q_value: -83.4114\n",
            "epoch: 12, episode: 15, Reward: -112.1185, distance: 55.3671, Critic Loss: 21242.5957, actor_q_value: -83.8922\n",
            "epoch: 13, episode: 0, Reward: -109.0872, distance: 54.5436, Critic Loss: 19894.3555, actor_q_value: -86.0519\n",
            "epoch: 13, episode: 5, Reward: -110.2844, distance: 55.0796, Critic Loss: 20254.3164, actor_q_value: -87.0428\n",
            "epoch: 13, episode: 10, Reward: -111.7190, distance: 55.4812, Critic Loss: 20712.7148, actor_q_value: -88.9990\n",
            "epoch: 13, episode: 15, Reward: -112.9487, distance: 55.7771, Critic Loss: 21090.0078, actor_q_value: -94.9317\n",
            "epoch: 14, episode: 0, Reward: -114.4395, distance: 57.2197, Critic Loss: 21576.6914, actor_q_value: -93.3095\n",
            "epoch: 14, episode: 5, Reward: -111.7135, distance: 55.7933, Critic Loss: 20330.0293, actor_q_value: -96.8131\n",
            "epoch: 14, episode: 10, Reward: -113.7507, distance: 56.4902, Critic Loss: 21027.3438, actor_q_value: -103.4478\n",
            "epoch: 14, episode: 15, Reward: -112.1571, distance: 55.3862, Critic Loss: 20246.7617, actor_q_value: -107.3895\n",
            "epoch: 15, episode: 0, Reward: -111.5329, distance: 55.7664, Critic Loss: 19863.3691, actor_q_value: -104.4597\n",
            "epoch: 15, episode: 5, Reward: -112.7693, distance: 56.3207, Critic Loss: 20222.8438, actor_q_value: -106.3047\n",
            "epoch: 15, episode: 10, Reward: -113.1150, distance: 56.1745, Critic Loss: 20224.1426, actor_q_value: -115.0420\n",
            "epoch: 15, episode: 15, Reward: -113.9336, distance: 56.2635, Critic Loss: 20414.1875, actor_q_value: -117.6786\n",
            "epoch: 16, episode: 0, Reward: -111.7258, distance: 55.8629, Critic Loss: 19392.6094, actor_q_value: -115.7760\n",
            "epoch: 16, episode: 5, Reward: -113.3266, distance: 56.5990, Critic Loss: 19885.1562, actor_q_value: -122.3124\n",
            "epoch: 16, episode: 10, Reward: -112.3994, distance: 55.8191, Critic Loss: 19374.4102, actor_q_value: -125.8685\n",
            "epoch: 16, episode: 15, Reward: -114.1250, distance: 56.3580, Critic Loss: 19913.0820, actor_q_value: -131.6259\n",
            "epoch: 17, episode: 0, Reward: -111.2846, distance: 55.6423, Critic Loss: 18651.4805, actor_q_value: -133.2326\n",
            "epoch: 17, episode: 5, Reward: -113.9387, distance: 56.9047, Critic Loss: 19541.3047, actor_q_value: -133.1532\n",
            "epoch: 17, episode: 10, Reward: -113.5496, distance: 56.3903, Critic Loss: 19238.0234, actor_q_value: -142.8687\n",
            "epoch: 17, episode: 15, Reward: -113.2632, distance: 55.9324, Critic Loss: 18974.9824, actor_q_value: -144.8442\n",
            "epoch: 18, episode: 0, Reward: -112.6005, distance: 56.3002, Critic Loss: 18566.5703, actor_q_value: -148.4971\n",
            "epoch: 18, episode: 5, Reward: -115.0167, distance: 57.4431, Critic Loss: 19353.2500, actor_q_value: -150.8825\n",
            "epoch: 18, episode: 10, Reward: -114.6444, distance: 56.9340, Critic Loss: 19050.5742, actor_q_value: -152.5022\n",
            "epoch: 18, episode: 15, Reward: -115.4829, distance: 57.0286, Critic Loss: 19219.6523, actor_q_value: -159.8314\n",
            "epoch: 19, episode: 0, Reward: -113.6461, distance: 56.8230, Critic Loss: 18348.1562, actor_q_value: -159.0531\n",
            "epoch: 19, episode: 5, Reward: -115.5481, distance: 57.7085, Critic Loss: 18921.4219, actor_q_value: -163.0461\n",
            "epoch: 19, episode: 10, Reward: -115.1765, distance: 57.1982, Critic Loss: 18614.4863, actor_q_value: -171.1541\n",
            "epoch: 19, episode: 15, Reward: -117.2067, distance: 57.8799, Critic Loss: 19238.6328, actor_q_value: -177.3530\n",
            "epoch: 20, episode: 0, Reward: -114.1797, distance: 57.0899, Critic Loss: 17905.7812, actor_q_value: -170.8460\n",
            "epoch: 20, episode: 5, Reward: -117.0226, distance: 58.4449, Critic Loss: 18828.1094, actor_q_value: -179.5021\n",
            "epoch: 20, episode: 10, Reward: -115.3380, distance: 57.2785, Critic Loss: 18012.6562, actor_q_value: -184.8627\n",
            "epoch: 20, episode: 15, Reward: -115.9381, distance: 57.2534, Critic Loss: 18071.5039, actor_q_value: -188.6864\n",
            "epoch: 21, episode: 0, Reward: -113.3450, distance: 56.6725, Critic Loss: 16932.9805, actor_q_value: -193.0301\n",
            "epoch: 21, episode: 5, Reward: -115.6997, distance: 57.7842, Critic Loss: 17639.3086, actor_q_value: -195.7329\n",
            "epoch: 21, episode: 10, Reward: -116.0440, distance: 57.6291, Critic Loss: 17596.2422, actor_q_value: -202.6313\n",
            "epoch: 21, episode: 15, Reward: -116.9587, distance: 57.7574, Critic Loss: 17765.8848, actor_q_value: -206.3963\n",
            "epoch: 22, episode: 0, Reward: -114.7952, distance: 57.3976, Critic Loss: 16787.5078, actor_q_value: -208.1387\n",
            "epoch: 22, episode: 5, Reward: -113.6888, distance: 56.7799, Critic Loss: 16213.4502, actor_q_value: -214.1664\n",
            "epoch: 22, episode: 10, Reward: -116.0118, distance: 57.6131, Critic Loss: 16885.2773, actor_q_value: -218.0251\n",
            "epoch: 22, episode: 15, Reward: -117.9906, distance: 58.2670, Critic Loss: 17439.7930, actor_q_value: -222.9028\n",
            "epoch: 23, episode: 0, Reward: -118.6387, distance: 59.3193, Critic Loss: 17499.7969, actor_q_value: -226.6117\n",
            "epoch: 23, episode: 5, Reward: -117.0925, distance: 58.4798, Critic Loss: 16744.8672, actor_q_value: -231.6146\n",
            "epoch: 23, episode: 10, Reward: -118.3539, distance: 58.7762, Critic Loss: 17026.1133, actor_q_value: -236.6393\n",
            "epoch: 23, episode: 15, Reward: -116.8637, distance: 57.7105, Critic Loss: 16299.0977, actor_q_value: -240.3124\n",
            "epoch: 24, episode: 0, Reward: -117.7831, distance: 58.8915, Critic Loss: 16448.8906, actor_q_value: -241.1215\n",
            "epoch: 24, episode: 5, Reward: -121.7931, distance: 60.8274, Critic Loss: 17742.8691, actor_q_value: -247.4474\n",
            "epoch: 24, episode: 10, Reward: -121.1743, distance: 60.1768, Critic Loss: 17318.1035, actor_q_value: -252.2578\n",
            "epoch: 24, episode: 15, Reward: -119.0099, distance: 58.7703, Critic Loss: 16332.7412, actor_q_value: -261.0691\n",
            "epoch: 25, episode: 0, Reward: -119.8099, distance: 59.9049, Critic Loss: 16431.8008, actor_q_value: -254.7464\n",
            "epoch: 25, episode: 5, Reward: -118.2081, distance: 59.0369, Critic Loss: 15667.3105, actor_q_value: -264.5069\n",
            "epoch: 25, episode: 10, Reward: -122.3311, distance: 60.7514, Critic Loss: 16963.4180, actor_q_value: -276.2070\n",
            "epoch: 25, episode: 15, Reward: -120.0260, distance: 59.2721, Critic Loss: 15931.1143, actor_q_value: -277.6336\n",
            "epoch: 26, episode: 0, Reward: -119.5452, distance: 59.7726, Critic Loss: 15566.7324, actor_q_value: -278.8104\n",
            "epoch: 26, episode: 5, Reward: -120.1222, distance: 59.9929, Critic Loss: 15576.0342, actor_q_value: -283.0091\n",
            "epoch: 26, episode: 10, Reward: -122.0973, distance: 60.6352, Critic Loss: 16081.8955, actor_q_value: -293.6210\n",
            "epoch: 26, episode: 15, Reward: -121.6403, distance: 60.0693, Critic Loss: 15719.8770, actor_q_value: -300.3847\n",
            "epoch: 27, episode: 0, Reward: -121.0395, distance: 60.5197, Critic Loss: 15309.7129, actor_q_value: -298.1467\n",
            "epoch: 27, episode: 5, Reward: -122.6152, distance: 61.2380, Critic Loss: 15664.0811, actor_q_value: -307.0455\n",
            "epoch: 27, episode: 10, Reward: -121.5089, distance: 60.3430, Critic Loss: 15075.4004, actor_q_value: -314.1104\n",
            "epoch: 27, episode: 15, Reward: -124.6356, distance: 61.5484, Critic Loss: 15973.6777, actor_q_value: -320.0953\n",
            "epoch: 28, episode: 0, Reward: -125.2939, distance: 62.6469, Critic Loss: 16000.6084, actor_q_value: -319.0838\n",
            "epoch: 28, episode: 5, Reward: -125.3063, distance: 62.5820, Critic Loss: 15795.2012, actor_q_value: -326.2721\n",
            "epoch: 28, episode: 10, Reward: -125.7003, distance: 62.4245, Critic Loss: 15724.8340, actor_q_value: -333.5428\n",
            "epoch: 28, episode: 15, Reward: -122.9627, distance: 60.7223, Critic Loss: 14564.4863, actor_q_value: -341.2903\n",
            "epoch: 29, episode: 0, Reward: -123.5438, distance: 61.7719, Critic Loss: 14558.3418, actor_q_value: -337.3439\n",
            "epoch: 29, episode: 5, Reward: -127.7179, distance: 63.7865, Critic Loss: 15801.8867, actor_q_value: -342.9359\n",
            "epoch: 29, episode: 10, Reward: -125.6868, distance: 62.4178, Critic Loss: 14876.7598, actor_q_value: -358.1713\n",
            "epoch: 29, episode: 15, Reward: -124.3179, distance: 61.3915, Critic Loss: 14200.5938, actor_q_value: -363.6506\n",
            "epoch: 30, episode: 0, Reward: -127.8961, distance: 63.9481, Critic Loss: 15215.6465, actor_q_value: -359.5942\n",
            "epoch: 30, episode: 5, Reward: -126.1078, distance: 62.9823, Critic Loss: 14385.5273, actor_q_value: -369.9120\n",
            "epoch: 30, episode: 10, Reward: -125.3496, distance: 62.2504, Critic Loss: 13919.0361, actor_q_value: -382.1875\n",
            "epoch: 30, episode: 15, Reward: -125.4124, distance: 61.9320, Critic Loss: 13730.0703, actor_q_value: -388.5789\n",
            "epoch: 31, episode: 0, Reward: -130.8412, distance: 65.4206, Critic Loss: 15365.0371, actor_q_value: -384.2518\n",
            "epoch: 31, episode: 5, Reward: -128.1756, distance: 64.0151, Critic Loss: 14228.1660, actor_q_value: -393.7800\n",
            "epoch: 31, episode: 10, Reward: -129.1695, distance: 64.1474, Critic Loss: 14346.2793, actor_q_value: -402.8269\n",
            "epoch: 31, episode: 15, Reward: -129.3496, distance: 63.8763, Critic Loss: 14187.5586, actor_q_value: -409.0810\n",
            "epoch: 32, episode: 0, Reward: -132.8710, distance: 66.4355, Critic Loss: 15169.6172, actor_q_value: -408.2925\n",
            "epoch: 32, episode: 5, Reward: -130.7062, distance: 65.2789, Critic Loss: 14201.1133, actor_q_value: -414.0462\n",
            "epoch: 32, episode: 10, Reward: -131.1485, distance: 65.1302, Critic Loss: 14125.8418, actor_q_value: -424.3267\n",
            "epoch: 32, episode: 15, Reward: -130.3486, distance: 64.3697, Critic Loss: 13635.8438, actor_q_value: -430.6625\n",
            "epoch: 33, episode: 0, Reward: -139.2335, distance: 69.6167, Critic Loss: 16482.6543, actor_q_value: -428.0118\n",
            "epoch: 33, episode: 5, Reward: -129.6066, distance: 64.7297, Critic Loss: 12952.4590, actor_q_value: -442.9995\n",
            "epoch: 33, episode: 10, Reward: -131.8526, distance: 65.4798, Critic Loss: 13460.3271, actor_q_value: -451.4572\n",
            "epoch: 33, episode: 15, Reward: -131.1084, distance: 64.7449, Critic Loss: 12994.4570, actor_q_value: -456.9114\n",
            "epoch: 34, episode: 0, Reward: -132.5456, distance: 66.2728, Critic Loss: 13235.4219, actor_q_value: -456.0274\n",
            "epoch: 34, episode: 5, Reward: -133.0624, distance: 66.4557, Critic Loss: 13176.0254, actor_q_value: -466.0280\n",
            "epoch: 34, episode: 10, Reward: -132.7417, distance: 65.9214, Critic Loss: 12845.5986, actor_q_value: -473.7407\n",
            "epoch: 34, episode: 15, Reward: -134.4874, distance: 66.4136, Critic Loss: 13179.7324, actor_q_value: -480.5049\n",
            "epoch: 35, episode: 0, Reward: -138.8250, distance: 69.4125, Critic Loss: 14381.4219, actor_q_value: -480.5378\n",
            "epoch: 35, episode: 5, Reward: -137.6497, distance: 68.7467, Critic Loss: 13744.3457, actor_q_value: -488.4556\n",
            "epoch: 35, episode: 10, Reward: -135.1426, distance: 67.1137, Critic Loss: 12692.0645, actor_q_value: -500.2662\n",
            "epoch: 35, episode: 15, Reward: -132.6951, distance: 65.5284, Critic Loss: 11699.0391, actor_q_value: -511.6157\n",
            "epoch: 36, episode: 0, Reward: -140.4753, distance: 70.2376, Critic Loss: 13953.8691, actor_q_value: -508.5232\n",
            "epoch: 36, episode: 5, Reward: -142.8126, distance: 71.3253, Critic Loss: 14492.2959, actor_q_value: -512.1019\n",
            "epoch: 36, episode: 10, Reward: -141.9005, distance: 70.4698, Critic Loss: 13931.5752, actor_q_value: -524.7599\n",
            "epoch: 36, episode: 15, Reward: -143.4224, distance: 70.8259, Critic Loss: 14188.3145, actor_q_value: -537.0826\n",
            "epoch: 37, episode: 0, Reward: -145.2538, distance: 72.6269, Critic Loss: 14551.0674, actor_q_value: -530.9384\n",
            "epoch: 37, episode: 5, Reward: -144.0428, distance: 71.9397, Critic Loss: 13883.9434, actor_q_value: -544.0637\n",
            "epoch: 37, episode: 10, Reward: -142.4082, distance: 70.7219, Critic Loss: 13093.0371, actor_q_value: -552.0106\n",
            "epoch: 37, episode: 15, Reward: -142.3806, distance: 70.3114, Critic Loss: 12833.8691, actor_q_value: -558.7515\n",
            "epoch: 38, episode: 0, Reward: -141.2650, distance: 70.6325, Critic Loss: 12232.9619, actor_q_value: -559.5300\n",
            "epoch: 38, episode: 5, Reward: -137.6990, distance: 68.7713, Critic Loss: 10909.4609, actor_q_value: -577.8457\n",
            "epoch: 38, episode: 10, Reward: -143.0641, distance: 71.0476, Critic Loss: 12303.5195, actor_q_value: -576.5039\n",
            "epoch: 38, episode: 15, Reward: -148.9947, distance: 73.5776, Critic Loss: 13967.0000, actor_q_value: -586.3758\n",
            "epoch: 39, episode: 0, Reward: -148.9678, distance: 74.4839, Critic Loss: 13689.2539, actor_q_value: -588.6962\n",
            "epoch: 39, episode: 5, Reward: -141.9482, distance: 70.8935, Critic Loss: 11219.0488, actor_q_value: -601.9822\n",
            "epoch: 39, episode: 10, Reward: -147.6650, distance: 73.3325, Critic Loss: 12735.6055, actor_q_value: -611.8498\n",
            "epoch: 39, episode: 15, Reward: -146.4667, distance: 72.3292, Critic Loss: 12099.4199, actor_q_value: -618.6544\n",
            "epoch: 40, episode: 0, Reward: -152.1095, distance: 76.0547, Critic Loss: 13643.5693, actor_q_value: -619.1454\n",
            "epoch: 40, episode: 5, Reward: -145.9199, distance: 72.8771, Critic Loss: 11420.6445, actor_q_value: -629.7046\n",
            "epoch: 40, episode: 10, Reward: -150.4244, distance: 74.7029, Critic Loss: 12554.3516, actor_q_value: -641.7471\n",
            "epoch: 40, episode: 15, Reward: -150.1475, distance: 74.1469, Critic Loss: 12200.7500, actor_q_value: -650.4301\n",
            "epoch: 41, episode: 0, Reward: -157.5894, distance: 78.7947, Critic Loss: 14347.0469, actor_q_value: -646.0472\n",
            "epoch: 41, episode: 5, Reward: -160.3136, distance: 80.0658, Critic Loss: 14985.5547, actor_q_value: -658.8524\n",
            "epoch: 41, episode: 10, Reward: -152.4462, distance: 75.7069, Critic Loss: 12113.6562, actor_q_value: -679.3615\n",
            "epoch: 41, episode: 15, Reward: -156.0067, distance: 77.0404, Critic Loss: 12965.9023, actor_q_value: -686.6893\n",
            "epoch: 42, episode: 0, Reward: -160.9722, distance: 80.4861, Critic Loss: 14318.0859, actor_q_value: -680.1614\n",
            "epoch: 42, episode: 5, Reward: -155.0865, distance: 77.4552, Critic Loss: 12120.7500, actor_q_value: -692.9777\n",
            "epoch: 42, episode: 10, Reward: -152.3017, distance: 75.6351, Critic Loss: 11006.5059, actor_q_value: -703.4077\n",
            "epoch: 42, episode: 15, Reward: -158.5404, distance: 78.2916, Critic Loss: 12653.0234, actor_q_value: -713.8317\n",
            "epoch: 43, episode: 0, Reward: -159.6165, distance: 79.8083, Critic Loss: 12711.6357, actor_q_value: -708.2542\n",
            "epoch: 43, episode: 5, Reward: -164.1237, distance: 81.9687, Critic Loss: 13886.2256, actor_q_value: -720.5717\n",
            "epoch: 43, episode: 10, Reward: -161.2236, distance: 80.0659, Critic Loss: 12642.0918, actor_q_value: -730.8073\n",
            "epoch: 43, episode: 15, Reward: -163.5906, distance: 80.7855, Critic Loss: 13104.5381, actor_q_value: -741.6724\n",
            "epoch: 44, episode: 0, Reward: -172.8190, distance: 86.4095, Critic Loss: 15927.5605, actor_q_value: -742.8412\n",
            "epoch: 44, episode: 5, Reward: -169.2510, distance: 84.5294, Critic Loss: 14356.1914, actor_q_value: -755.1057\n",
            "epoch: 44, episode: 10, Reward: -167.5653, distance: 83.2153, Critic Loss: 13472.8711, actor_q_value: -764.3801\n",
            "epoch: 44, episode: 15, Reward: -176.2218, distance: 87.0231, Critic Loss: 16113.5273, actor_q_value: -775.7808\n",
            "epoch: 45, episode: 0, Reward: -169.6617, distance: 84.8308, Critic Loss: 13518.7031, actor_q_value: -778.4269\n",
            "epoch: 45, episode: 5, Reward: -169.0672, distance: 84.4376, Critic Loss: 13001.7871, actor_q_value: -790.2850\n",
            "epoch: 45, episode: 10, Reward: -171.0792, distance: 84.9603, Critic Loss: 13332.0146, actor_q_value: -807.8371\n",
            "epoch: 45, episode: 15, Reward: -171.3190, distance: 84.6020, Critic Loss: 13086.6621, actor_q_value: -820.9855\n",
            "epoch: 46, episode: 0, Reward: -175.2739, distance: 87.6369, Critic Loss: 14060.1445, actor_q_value: -816.3014\n",
            "epoch: 46, episode: 5, Reward: -177.7831, distance: 88.7906, Critic Loss: 14568.7383, actor_q_value: -825.6133\n",
            "epoch: 46, episode: 10, Reward: -176.5887, distance: 87.6964, Critic Loss: 13825.4297, actor_q_value: -841.9811\n",
            "epoch: 46, episode: 15, Reward: -176.6874, distance: 87.2531, Critic Loss: 13521.7871, actor_q_value: -853.0215\n",
            "epoch: 47, episode: 0, Reward: -184.9172, distance: 92.4586, Critic Loss: 15995.7148, actor_q_value: -852.2786\n",
            "epoch: 47, episode: 5, Reward: -185.4799, distance: 92.6347, Critic Loss: 15823.5098, actor_q_value: -861.0509\n",
            "epoch: 47, episode: 10, Reward: -187.2560, distance: 92.9939, Critic Loss: 16076.3242, actor_q_value: -872.4621\n",
            "epoch: 47, episode: 15, Reward: -183.1424, distance: 90.4407, Critic Loss: 14265.4102, actor_q_value: -883.7545\n",
            "epoch: 48, episode: 0, Reward: -181.7833, distance: 90.8916, Critic Loss: 13448.2363, actor_q_value: -900.8809\n",
            "epoch: 48, episode: 5, Reward: -182.9165, distance: 91.3544, Critic Loss: 13459.6406, actor_q_value: -903.6710\n",
            "epoch: 48, episode: 10, Reward: -187.0661, distance: 92.8996, Critic Loss: 14477.8877, actor_q_value: -923.4259\n",
            "epoch: 48, episode: 15, Reward: -192.5518, distance: 95.0873, Critic Loss: 16004.8086, actor_q_value: -927.2432\n",
            "epoch: 49, episode: 0, Reward: -192.5605, distance: 96.2803, Critic Loss: 15609.9980, actor_q_value: -935.1996\n",
            "epoch: 49, episode: 5, Reward: -194.0144, distance: 96.8971, Critic Loss: 15725.4541, actor_q_value: -943.4866\n",
            "epoch: 49, episode: 10, Reward: -187.3067, distance: 93.0191, Critic Loss: 13068.0723, actor_q_value: -958.0311\n",
            "epoch: 49, episode: 15, Reward: -185.2946, distance: 91.5035, Critic Loss: 12071.6641, actor_q_value: -974.0056\n",
            "epoch: 50, episode: 0, Reward: -199.7309, distance: 99.8655, Critic Loss: 16560.9824, actor_q_value: -974.4059\n",
            "epoch: 50, episode: 5, Reward: -197.2222, distance: 98.4992, Critic Loss: 15258.3770, actor_q_value: -985.9893\n",
            "epoch: 50, episode: 10, Reward: -196.4227, distance: 97.5462, Critic Loss: 14581.8008, actor_q_value: -998.5433\n",
            "epoch: 50, episode: 15, Reward: -200.9336, distance: 99.2265, Critic Loss: 15742.7812, actor_q_value: -1009.2955\n",
            "epoch: 51, episode: 0, Reward: -197.5803, distance: 98.7902, Critic Loss: 14171.7188, actor_q_value: -1019.5146\n",
            "epoch: 51, episode: 5, Reward: -199.9755, distance: 99.8742, Critic Loss: 14577.8945, actor_q_value: -1033.0293\n",
            "epoch: 51, episode: 10, Reward: -198.8725, distance: 98.7628, Critic Loss: 13801.4238, actor_q_value: -1045.2487\n",
            "epoch: 51, episode: 15, Reward: -200.0981, distance: 98.8139, Critic Loss: 13806.8066, actor_q_value: -1056.2629\n",
            "epoch: 52, episode: 0, Reward: -210.2513, distance: 105.1257, Critic Loss: 16935.5820, actor_q_value: -1062.3422\n",
            "epoch: 52, episode: 5, Reward: -203.9731, distance: 101.8708, Critic Loss: 14286.5234, actor_q_value: -1075.1187\n",
            "epoch: 52, episode: 10, Reward: -214.3253, distance: 106.4369, Critic Loss: 17536.8672, actor_q_value: -1087.7794\n",
            "epoch: 52, episode: 15, Reward: -211.7401, distance: 104.5630, Critic Loss: 16134.2891, actor_q_value: -1102.1499\n",
            "epoch: 53, episode: 0, Reward: -214.2153, distance: 107.1077, Critic Loss: 16581.1270, actor_q_value: -1106.1638\n",
            "epoch: 53, episode: 5, Reward: -214.0057, distance: 106.8814, Critic Loss: 16052.3125, actor_q_value: -1120.4635\n",
            "epoch: 53, episode: 10, Reward: -218.0904, distance: 108.3067, Critic Loss: 17078.3711, actor_q_value: -1134.1343\n",
            "epoch: 53, episode: 15, Reward: -217.6908, distance: 107.5016, Critic Loss: 16466.4082, actor_q_value: -1157.5968\n",
            "epoch: 54, episode: 0, Reward: -222.5479, distance: 111.2740, Critic Loss: 17791.0703, actor_q_value: -1153.3912\n",
            "epoch: 54, episode: 5, Reward: -224.7864, distance: 112.2656, Critic Loss: 18148.1094, actor_q_value: -1167.7314\n",
            "epoch: 54, episode: 10, Reward: -214.9784, distance: 106.7613, Critic Loss: 14160.5078, actor_q_value: -1187.0258\n",
            "epoch: 54, episode: 15, Reward: -213.2106, distance: 105.2892, Critic Loss: 13145.8057, actor_q_value: -1200.2860\n",
            "epoch: 55, episode: 0, Reward: -231.3227, distance: 115.6614, Critic Loss: 19159.9258, actor_q_value: -1203.4150\n",
            "epoch: 55, episode: 5, Reward: -226.6878, distance: 113.2152, Critic Loss: 16894.4727, actor_q_value: -1214.7673\n",
            "epoch: 55, episode: 10, Reward: -223.5847, distance: 111.0353, Critic Loss: 15298.2910, actor_q_value: -1238.3444\n",
            "epoch: 55, episode: 15, Reward: -217.1939, distance: 107.2563, Critic Loss: 12710.2100, actor_q_value: -1252.7510\n",
            "epoch: 56, episode: 0, Reward: -230.7187, distance: 115.3594, Critic Loss: 16890.3008, actor_q_value: -1251.6171\n",
            "epoch: 56, episode: 5, Reward: -230.1034, distance: 114.9211, Critic Loss: 16172.8340, actor_q_value: -1264.7305\n",
            "epoch: 56, episode: 10, Reward: -226.5951, distance: 112.5303, Critic Loss: 14469.3809, actor_q_value: -1295.7610\n",
            "epoch: 56, episode: 15, Reward: -232.0834, distance: 114.6091, Critic Loss: 15907.2812, actor_q_value: -1302.1189\n",
            "epoch: 57, episode: 0, Reward: -237.5600, distance: 118.7800, Critic Loss: 17402.1797, actor_q_value: -1303.0883\n",
            "epoch: 57, episode: 5, Reward: -244.7577, distance: 122.2399, Critic Loss: 19628.0820, actor_q_value: -1317.8984\n",
            "epoch: 57, episode: 10, Reward: -249.5791, distance: 123.9445, Critic Loss: 20993.3906, actor_q_value: -1334.3093\n",
            "epoch: 57, episode: 15, Reward: -249.5345, distance: 123.2269, Critic Loss: 20386.8789, actor_q_value: -1347.4833\n",
            "epoch: 58, episode: 0, Reward: -240.9544, distance: 120.4772, Critic Loss: 16530.6621, actor_q_value: -1358.8214\n",
            "epoch: 58, episode: 5, Reward: -248.2609, distance: 123.9896, Critic Loss: 18715.3359, actor_q_value: -1373.7190\n",
            "epoch: 58, episode: 10, Reward: -248.2937, distance: 123.3061, Critic Loss: 18149.1914, actor_q_value: -1386.8954\n",
            "epoch: 58, episode: 15, Reward: -239.8395, distance: 118.4393, Critic Loss: 14550.3125, actor_q_value: -1409.4587\n",
            "epoch: 59, episode: 0, Reward: -251.9108, distance: 125.9554, Critic Loss: 18375.2500, actor_q_value: -1413.5935\n",
            "epoch: 59, episode: 5, Reward: -256.1920, distance: 127.9506, Critic Loss: 19442.9434, actor_q_value: -1427.7020\n",
            "epoch: 59, episode: 10, Reward: -263.9391, distance: 131.0759, Critic Loss: 21960.1602, actor_q_value: -1444.5548\n",
            "epoch: 59, episode: 15, Reward: -255.7149, distance: 126.2789, Critic Loss: 18045.9941, actor_q_value: -1465.5493\n",
            "epoch: 60, episode: 0, Reward: -273.8621, distance: 136.9310, Critic Loss: 24893.1562, actor_q_value: -1473.3130\n",
            "epoch: 60, episode: 5, Reward: -271.5621, distance: 135.6269, Critic Loss: 23190.4766, actor_q_value: -1487.2605\n",
            "epoch: 60, episode: 10, Reward: -266.3763, distance: 132.2862, Critic Loss: 20356.4375, actor_q_value: -1507.2531\n",
            "epoch: 60, episode: 15, Reward: -269.8070, distance: 133.2380, Critic Loss: 21094.4805, actor_q_value: -1521.0698\n",
            "epoch: 61, episode: 0, Reward: -271.7039, distance: 135.8519, Critic Loss: 21208.1875, actor_q_value: -1531.9961\n",
            "epoch: 61, episode: 5, Reward: -273.4524, distance: 136.5710, Critic Loss: 21254.9492, actor_q_value: -1549.9401\n",
            "epoch: 61, episode: 10, Reward: -272.4458, distance: 135.3004, Critic Loss: 20172.5391, actor_q_value: -1568.3247\n",
            "epoch: 61, episode: 15, Reward: -273.6512, distance: 135.1364, Critic Loss: 19991.1680, actor_q_value: -1585.8236\n",
            "epoch: 62, episode: 0, Reward: -282.8294, distance: 141.4147, Critic Loss: 23110.7441, actor_q_value: -1598.1930\n",
            "epoch: 62, episode: 5, Reward: -285.7635, distance: 142.7196, Critic Loss: 23654.5352, actor_q_value: -1612.6306\n",
            "epoch: 62, episode: 10, Reward: -286.7863, distance: 142.4221, Critic Loss: 23362.8594, actor_q_value: -1629.3236\n",
            "epoch: 62, episode: 15, Reward: -283.1156, distance: 139.8102, Critic Loss: 21093.8633, actor_q_value: -1646.0377\n",
            "epoch: 63, episode: 0, Reward: -288.1066, distance: 144.0533, Critic Loss: 22455.3711, actor_q_value: -1660.4108\n",
            "epoch: 63, episode: 5, Reward: -289.2744, distance: 144.4730, Critic Loss: 22213.3594, actor_q_value: -1677.5651\n",
            "epoch: 63, episode: 10, Reward: -290.5595, distance: 144.2959, Critic Loss: 22017.7695, actor_q_value: -1696.2163\n",
            "epoch: 63, episode: 15, Reward: -295.7365, distance: 146.0427, Critic Loss: 23470.9961, actor_q_value: -1714.1627\n",
            "epoch: 64, episode: 0, Reward: -298.9866, distance: 149.4933, Critic Loss: 24112.1387, actor_q_value: -1727.0208\n",
            "epoch: 64, episode: 5, Reward: -306.2435, distance: 152.9479, Critic Loss: 26570.9258, actor_q_value: -1744.4569\n",
            "epoch: 64, episode: 10, Reward: -295.4492, distance: 146.7242, Critic Loss: 21081.0117, actor_q_value: -1765.5736\n",
            "epoch: 64, episode: 15, Reward: -293.3753, distance: 144.8767, Critic Loss: 19525.1895, actor_q_value: -1785.4685\n",
            "epoch: 65, episode: 0, Reward: -303.7117, distance: 151.8558, Critic Loss: 23055.5039, actor_q_value: -1796.5112\n",
            "epoch: 65, episode: 5, Reward: -321.0725, distance: 160.3540, Critic Loss: 30227.0938, actor_q_value: -1813.2433\n",
            "epoch: 65, episode: 10, Reward: -310.7293, distance: 154.3125, Critic Loss: 24541.8828, actor_q_value: -1832.3206\n",
            "epoch: 65, episode: 15, Reward: -306.7837, distance: 151.4981, Critic Loss: 22053.4180, actor_q_value: -1852.7690\n",
            "epoch: 66, episode: 0, Reward: -306.7853, distance: 153.3926, Critic Loss: 21295.4570, actor_q_value: -1865.2228\n",
            "epoch: 66, episode: 5, Reward: -320.6963, distance: 160.1661, Critic Loss: 26571.1328, actor_q_value: -1882.4225\n",
            "epoch: 66, episode: 10, Reward: -322.6264, distance: 160.2208, Critic Loss: 26606.6953, actor_q_value: -1901.0439\n",
            "epoch: 66, episode: 15, Reward: -314.2901, distance: 155.2050, Critic Loss: 22105.7051, actor_q_value: -1921.6846\n",
            "epoch: 67, episode: 0, Reward: -317.4935, distance: 158.7467, Critic Loss: 22666.6172, actor_q_value: -1937.2296\n",
            "epoch: 67, episode: 5, Reward: -323.9431, distance: 161.7877, Critic Loss: 24653.0000, actor_q_value: -1955.0953\n",
            "epoch: 67, episode: 10, Reward: -320.8664, distance: 159.3468, Critic Loss: 22496.5801, actor_q_value: -1973.8097\n",
            "epoch: 67, episode: 15, Reward: -326.2914, distance: 161.1315, Critic Loss: 24016.4941, actor_q_value: -1993.9482\n",
            "epoch: 68, episode: 0, Reward: -324.7513, distance: 162.3756, Critic Loss: 22526.3711, actor_q_value: -2009.9332\n",
            "epoch: 68, episode: 5, Reward: -335.6047, distance: 167.6119, Critic Loss: 26475.6914, actor_q_value: -2027.5544\n",
            "epoch: 68, episode: 10, Reward: -339.4467, distance: 168.5740, Critic Loss: 27358.9531, actor_q_value: -2044.7588\n",
            "epoch: 68, episode: 15, Reward: -346.8435, distance: 171.2807, Critic Loss: 29967.0312, actor_q_value: -2067.9553\n",
            "epoch: 69, episode: 0, Reward: -343.9631, distance: 171.9816, Critic Loss: 27641.2441, actor_q_value: -2082.3884\n",
            "epoch: 69, episode: 5, Reward: -346.9425, distance: 173.2743, Critic Loss: 28109.0469, actor_q_value: -2105.1157\n",
            "epoch: 69, episode: 10, Reward: -338.2480, distance: 167.9787, Critic Loss: 23269.7656, actor_q_value: -2124.0549\n",
            "epoch: 69, episode: 15, Reward: -352.0114, distance: 173.8328, Critic Loss: 28621.2930, actor_q_value: -2142.5029\n",
            "epoch: 70, episode: 0, Reward: -350.9983, distance: 175.4992, Critic Loss: 27190.0742, actor_q_value: -2161.4888\n",
            "epoch: 70, episode: 5, Reward: -354.6968, distance: 177.1471, Critic Loss: 27977.0918, actor_q_value: -2181.1223\n",
            "epoch: 70, episode: 10, Reward: -358.9865, distance: 178.2777, Critic Loss: 29053.2031, actor_q_value: -2200.6287\n",
            "epoch: 70, episode: 15, Reward: -360.7371, distance: 178.1418, Critic Loss: 28911.2109, actor_q_value: -2218.3789\n",
            "epoch: 71, episode: 0, Reward: -361.2855, distance: 180.6427, Critic Loss: 28186.7168, actor_q_value: -2239.5088\n",
            "epoch: 71, episode: 5, Reward: -361.1655, distance: 180.3777, Critic Loss: 27154.4375, actor_q_value: -2260.9534\n",
            "epoch: 71, episode: 10, Reward: -341.1309, distance: 169.4104, Critic Loss: 17839.7266, actor_q_value: -2302.9167\n",
            "epoch: 71, episode: 15, Reward: -347.3204, distance: 171.5162, Critic Loss: 19462.2812, actor_q_value: -2317.9651\n",
            "epoch: 72, episode: 0, Reward: -374.4092, distance: 187.2046, Critic Loss: 30640.5469, actor_q_value: -2318.4895\n",
            "epoch: 72, episode: 5, Reward: -368.5713, distance: 184.0765, Critic Loss: 26893.8281, actor_q_value: -2333.7039\n",
            "epoch: 72, episode: 10, Reward: -376.9344, distance: 187.1909, Critic Loss: 29926.3633, actor_q_value: -2357.6345\n",
            "epoch: 72, episode: 15, Reward: -367.0966, distance: 181.2823, Critic Loss: 24402.8555, actor_q_value: -2377.1848\n",
            "epoch: 73, episode: 0, Reward: -376.6436, distance: 188.3218, Critic Loss: 27830.7891, actor_q_value: -2393.7988\n",
            "epoch: 73, episode: 5, Reward: -382.0646, distance: 190.8154, Critic Loss: 29442.4258, actor_q_value: -2413.6003\n",
            "epoch: 73, episode: 10, Reward: -369.6171, distance: 183.5570, Critic Loss: 22830.2656, actor_q_value: -2449.5652\n",
            "epoch: 73, episode: 15, Reward: -371.3274, distance: 183.3715, Critic Loss: 22692.9395, actor_q_value: -2457.1167\n",
            "epoch: 74, episode: 0, Reward: -393.7943, distance: 196.8971, Critic Loss: 32234.5918, actor_q_value: -2472.8735\n",
            "epoch: 74, episode: 5, Reward: -383.6875, distance: 191.6260, Critic Loss: 26356.6875, actor_q_value: -2493.7131\n",
            "epoch: 74, episode: 10, Reward: -397.1446, distance: 197.2276, Critic Loss: 31841.0469, actor_q_value: -2512.3083\n",
            "epoch: 74, episode: 15, Reward: -393.8065, distance: 194.4723, Critic Loss: 29152.2578, actor_q_value: -2533.3088\n",
            "epoch: 75, episode: 0, Reward: -403.2267, distance: 201.6134, Critic Loss: 32778.1523, actor_q_value: -2555.2285\n",
            "epoch: 75, episode: 5, Reward: -397.0154, distance: 198.2824, Critic Loss: 28634.8457, actor_q_value: -2575.3730\n",
            "epoch: 75, episode: 10, Reward: -410.8007, distance: 204.0094, Critic Loss: 34472.8750, actor_q_value: -2595.4849\n",
            "epoch: 75, episode: 15, Reward: -397.0196, distance: 196.0591, Critic Loss: 26607.6445, actor_q_value: -2619.0337\n",
            "epoch: 76, episode: 0, Reward: -419.5471, distance: 209.7736, Critic Loss: 36833.0156, actor_q_value: -2636.6973\n",
            "epoch: 76, episode: 5, Reward: -397.7623, distance: 198.6554, Critic Loss: 24982.8770, actor_q_value: -2658.2395\n",
            "epoch: 76, episode: 10, Reward: -384.1798, distance: 190.7891, Critic Loss: 18447.2207, actor_q_value: -2690.1216\n",
            "epoch: 76, episode: 15, Reward: -404.6667, distance: 199.8354, Critic Loss: 26173.6133, actor_q_value: -2705.1389\n",
            "epoch: 77, episode: 0, Reward: -401.9927, distance: 200.9964, Critic Loss: 24019.4785, actor_q_value: -2720.1431\n",
            "epoch: 77, episode: 5, Reward: -414.3951, distance: 206.9624, Critic Loss: 28727.6953, actor_q_value: -2742.2153\n",
            "epoch: 77, episode: 10, Reward: -415.4659, distance: 206.3262, Critic Loss: 28202.4297, actor_q_value: -2763.9771\n",
            "epoch: 77, episode: 15, Reward: -427.7026, distance: 211.2112, Critic Loss: 33170.1562, actor_q_value: -2784.6785\n",
            "epoch: 78, episode: 0, Reward: -423.6681, distance: 211.8341, Critic Loss: 30020.2656, actor_q_value: -2803.2568\n",
            "epoch: 78, episode: 5, Reward: -420.8901, distance: 210.2062, Critic Loss: 27604.0918, actor_q_value: -2827.5461\n",
            "epoch: 78, episode: 10, Reward: -425.3066, distance: 211.2132, Critic Loss: 28623.0449, actor_q_value: -2849.6792\n",
            "epoch: 78, episode: 15, Reward: -437.2900, distance: 215.9457, Critic Loss: 33451.6094, actor_q_value: -2871.8662\n",
            "epoch: 79, episode: 0, Reward: -428.6270, distance: 214.3135, Critic Loss: 28016.9219, actor_q_value: -2894.2595\n",
            "epoch: 79, episode: 5, Reward: -430.0907, distance: 214.8012, Critic Loss: 27623.4219, actor_q_value: -2916.3083\n",
            "epoch: 79, episode: 10, Reward: -425.5481, distance: 211.3332, Critic Loss: 24514.6250, actor_q_value: -2945.1819\n",
            "epoch: 79, episode: 15, Reward: -430.7775, distance: 212.7296, Critic Loss: 25860.0117, actor_q_value: -2960.5613\n",
            "epoch: 80, episode: 0, Reward: -440.1197, distance: 220.0598, Critic Loss: 29187.8066, actor_q_value: -2983.2302\n",
            "epoch: 80, episode: 5, Reward: -443.4485, distance: 221.4725, Critic Loss: 29702.4180, actor_q_value: -3005.1204\n",
            "epoch: 80, episode: 10, Reward: -449.1368, distance: 223.0476, Critic Loss: 31380.9844, actor_q_value: -3028.0820\n",
            "epoch: 80, episode: 15, Reward: -442.1810, distance: 218.3610, Critic Loss: 26905.2383, actor_q_value: -3048.8542\n",
            "epoch: 81, episode: 0, Reward: -453.1003, distance: 226.5501, Critic Loss: 31029.7852, actor_q_value: -3071.3428\n",
            "epoch: 81, episode: 5, Reward: -452.7384, distance: 226.1122, Critic Loss: 29689.0918, actor_q_value: -3094.6082\n",
            "epoch: 81, episode: 10, Reward: -455.3525, distance: 226.1344, Critic Loss: 29810.2285, actor_q_value: -3118.0737\n",
            "epoch: 81, episode: 15, Reward: -454.5423, distance: 224.4653, Critic Loss: 28279.5625, actor_q_value: -3143.2727\n",
            "epoch: 82, episode: 0, Reward: -449.3322, distance: 224.6661, Critic Loss: 24793.7188, actor_q_value: -3161.7361\n",
            "epoch: 82, episode: 5, Reward: -466.0002, distance: 232.7356, Critic Loss: 31576.2910, actor_q_value: -3185.2068\n",
            "epoch: 82, episode: 10, Reward: -453.9938, distance: 225.4597, Critic Loss: 24770.1504, actor_q_value: -3212.0330\n",
            "epoch: 82, episode: 15, Reward: -463.3476, distance: 228.8136, Critic Loss: 27982.9727, actor_q_value: -3234.0051\n",
            "epoch: 83, episode: 0, Reward: -470.3681, distance: 235.1841, Critic Loss: 30226.2480, actor_q_value: -3257.3701\n",
            "epoch: 83, episode: 5, Reward: -471.9012, distance: 235.6827, Critic Loss: 29814.5039, actor_q_value: -3278.7407\n",
            "epoch: 83, episode: 10, Reward: -481.1823, distance: 238.9619, Critic Loss: 33284.1758, actor_q_value: -3302.9260\n",
            "epoch: 83, episode: 15, Reward: -471.0298, distance: 232.6073, Critic Loss: 27126.3281, actor_q_value: -3326.7307\n",
            "epoch: 84, episode: 0, Reward: -478.3774, distance: 239.1887, Critic Loss: 29476.0625, actor_q_value: -3350.2544\n",
            "epoch: 84, episode: 5, Reward: -466.9170, distance: 233.1935, Critic Loss: 23130.9609, actor_q_value: -3372.9512\n",
            "epoch: 84, episode: 10, Reward: -484.1140, distance: 240.4178, Critic Loss: 29938.4160, actor_q_value: -3394.7207\n",
            "epoch: 84, episode: 15, Reward: -480.7953, distance: 237.4298, Critic Loss: 27211.4648, actor_q_value: -3420.9570\n",
            "epoch: 85, episode: 0, Reward: -487.9120, distance: 243.9560, Critic Loss: 29460.4609, actor_q_value: -3442.9971\n",
            "epoch: 85, episode: 5, Reward: -476.4897, distance: 237.9744, Critic Loss: 23135.2305, actor_q_value: -3466.1265\n",
            "epoch: 85, episode: 10, Reward: -486.7903, distance: 241.7469, Critic Loss: 26661.9922, actor_q_value: -3491.9312\n",
            "epoch: 85, episode: 15, Reward: -496.3664, distance: 245.1192, Critic Loss: 30085.5312, actor_q_value: -3514.2446\n",
            "epoch: 86, episode: 0, Reward: -502.4962, distance: 251.2481, Critic Loss: 31938.9844, actor_q_value: -3537.7622\n",
            "epoch: 86, episode: 5, Reward: -490.7714, distance: 245.1072, Critic Loss: 25194.3828, actor_q_value: -3559.7344\n",
            "epoch: 86, episode: 10, Reward: -494.8418, distance: 245.7454, Critic Loss: 25948.4082, actor_q_value: -3583.8972\n",
            "epoch: 86, episode: 15, Reward: -501.0294, distance: 247.4219, Critic Loss: 27695.6543, actor_q_value: -3607.6763\n",
            "epoch: 87, episode: 0, Reward: -501.5493, distance: 250.7746, Critic Loss: 26806.2129, actor_q_value: -3630.3328\n",
            "epoch: 87, episode: 5, Reward: -497.9305, distance: 248.6826, Critic Loss: 24081.9941, actor_q_value: -3653.0962\n",
            "epoch: 87, episode: 10, Reward: -505.0165, distance: 250.7983, Critic Loss: 26179.1230, actor_q_value: -3678.6494\n",
            "epoch: 87, episode: 15, Reward: -515.5280, distance: 254.5817, Critic Loss: 30018.4277, actor_q_value: -3700.7217\n",
            "epoch: 88, episode: 0, Reward: -517.0389, distance: 258.5194, Critic Loss: 29567.4688, actor_q_value: -3726.4785\n",
            "epoch: 88, episode: 5, Reward: -521.9524, distance: 260.6800, Critic Loss: 30781.5781, actor_q_value: -3749.5273\n",
            "epoch: 88, episode: 10, Reward: -516.4772, distance: 256.4898, Critic Loss: 26978.7363, actor_q_value: -3772.0845\n",
            "epoch: 88, episode: 15, Reward: -536.4050, distance: 264.8913, Critic Loss: 35704.2812, actor_q_value: -3798.1250\n",
            "epoch: 89, episode: 0, Reward: -524.6441, distance: 262.3221, Critic Loss: 28496.5566, actor_q_value: -3822.9065\n",
            "epoch: 89, episode: 5, Reward: -521.0459, distance: 260.2272, Critic Loss: 25662.4453, actor_q_value: -3846.9604\n",
            "epoch: 89, episode: 10, Reward: -535.8309, distance: 266.1011, Critic Loss: 31542.4648, actor_q_value: -3869.7568\n",
            "epoch: 89, episode: 15, Reward: -538.4297, distance: 265.8912, Critic Loss: 31601.9609, actor_q_value: -3893.1370\n",
            "epoch: 90, episode: 0, Reward: -519.2785, distance: 259.6393, Critic Loss: 21659.9141, actor_q_value: -3916.9683\n",
            "epoch: 90, episode: 5, Reward: -531.8668, distance: 265.6315, Critic Loss: 26076.8281, actor_q_value: -3942.8232\n",
            "epoch: 90, episode: 10, Reward: -537.7007, distance: 267.0297, Critic Loss: 27644.4805, actor_q_value: -3966.6260\n",
            "epoch: 90, episode: 15, Reward: -544.8239, distance: 269.0488, Critic Loss: 29893.7305, actor_q_value: -3990.0742\n",
            "epoch: 91, episode: 0, Reward: -528.4135, distance: 264.2067, Critic Loss: 21384.6230, actor_q_value: -4014.5852\n",
            "epoch: 91, episode: 5, Reward: -538.8148, distance: 269.1016, Critic Loss: 24805.6719, actor_q_value: -4039.8169\n",
            "epoch: 91, episode: 10, Reward: -513.0898, distance: 254.8076, Critic Loss: 13883.6816, actor_q_value: -4091.4094\n",
            "epoch: 91, episode: 15, Reward: -522.3848, distance: 257.9678, Critic Loss: 16337.2744, actor_q_value: -4098.5547\n",
            "epoch: 92, episode: 0, Reward: -540.9258, distance: 270.4629, Critic Loss: 22779.1191, actor_q_value: -4104.4277\n",
            "epoch: 92, episode: 5, Reward: -556.6783, distance: 278.0232, Critic Loss: 28923.1250, actor_q_value: -4126.0713\n",
            "epoch: 92, episode: 10, Reward: -549.5111, distance: 272.8949, Critic Loss: 24569.6211, actor_q_value: -4150.9688\n",
            "epoch: 92, episode: 15, Reward: -563.7814, distance: 278.4106, Critic Loss: 30159.9570, actor_q_value: -4172.3252\n",
            "epoch: 93, episode: 0, Reward: -541.8773, distance: 270.9387, Critic Loss: 19423.3789, actor_q_value: -4193.7974\n",
            "epoch: 93, episode: 5, Reward: -556.0832, distance: 277.7260, Critic Loss: 24369.2383, actor_q_value: -4218.1172\n",
            "epoch: 93, episode: 10, Reward: -559.6680, distance: 277.9390, Critic Loss: 24909.9922, actor_q_value: -4238.2749\n",
            "epoch: 93, episode: 15, Reward: -558.6740, distance: 275.8884, Critic Loss: 23423.4844, actor_q_value: -4265.7764\n",
            "epoch: 94, episode: 0, Reward: -562.8544, distance: 281.4272, Critic Loss: 24197.0430, actor_q_value: -4288.7383\n",
            "epoch: 94, episode: 5, Reward: -569.1299, distance: 284.2419, Critic Loss: 25916.8887, actor_q_value: -4311.4570\n",
            "epoch: 94, episode: 10, Reward: -573.1749, distance: 284.6467, Critic Loss: 26648.1875, actor_q_value: -4335.1436\n",
            "epoch: 94, episode: 15, Reward: -569.2189, distance: 281.0958, Critic Loss: 23760.5977, actor_q_value: -4359.8599\n",
            "epoch: 95, episode: 0, Reward: -562.6730, distance: 281.3365, Critic Loss: 19985.3086, actor_q_value: -4384.3418\n",
            "epoch: 95, episode: 5, Reward: -576.7444, distance: 288.0449, Critic Loss: 24883.4297, actor_q_value: -4406.6289\n",
            "epoch: 95, episode: 10, Reward: -584.6526, distance: 290.3466, Critic Loss: 27353.8711, actor_q_value: -4431.8447\n",
            "epoch: 95, episode: 15, Reward: -588.6091, distance: 290.6711, Critic Loss: 28029.0605, actor_q_value: -4460.8652\n",
            "epoch: 96, episode: 0, Reward: -576.1417, distance: 288.0708, Critic Loss: 21372.3594, actor_q_value: -4479.7847\n",
            "epoch: 96, episode: 5, Reward: -582.7340, distance: 291.0363, Critic Loss: 23087.1875, actor_q_value: -4506.5093\n",
            "epoch: 96, episode: 10, Reward: -573.4825, distance: 284.7994, Critic Loss: 18324.6250, actor_q_value: -4534.7822\n",
            "epoch: 96, episode: 15, Reward: -592.2577, distance: 292.4729, Critic Loss: 25110.9297, actor_q_value: -4560.0908\n",
            "epoch: 97, episode: 0, Reward: -584.9427, distance: 292.4713, Critic Loss: 20936.0820, actor_q_value: -4578.6094\n",
            "epoch: 97, episode: 5, Reward: -591.5832, distance: 295.4558, Critic Loss: 22693.0273, actor_q_value: -4603.8472\n",
            "epoch: 97, episode: 10, Reward: -591.5687, distance: 293.7813, Critic Loss: 21661.6035, actor_q_value: -4627.7046\n",
            "epoch: 97, episode: 15, Reward: -605.3936, distance: 298.9598, Critic Loss: 26660.0938, actor_q_value: -4650.8408\n",
            "epoch: 98, episode: 0, Reward: -592.7380, distance: 296.3690, Critic Loss: 20133.6719, actor_q_value: -4675.6943\n",
            "epoch: 98, episode: 5, Reward: -595.3353, distance: 297.3298, Critic Loss: 20187.4785, actor_q_value: -4698.9619\n",
            "epoch: 98, episode: 10, Reward: -608.2849, distance: 302.0828, Critic Loss: 24620.8301, actor_q_value: -4724.4658\n",
            "epoch: 98, episode: 15, Reward: -606.5272, distance: 299.5196, Critic Loss: 22772.9023, actor_q_value: -4749.8223\n",
            "epoch: 99, episode: 0, Reward: -613.2489, distance: 306.6245, Critic Loss: 24608.1523, actor_q_value: -4773.9229\n",
            "epoch: 99, episode: 5, Reward: -606.0369, distance: 302.6745, Critic Loss: 20490.3496, actor_q_value: -4798.0137\n",
            "epoch: 99, episode: 10, Reward: -605.9136, distance: 300.9052, Critic Loss: 19456.4922, actor_q_value: -4823.5728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reward"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ob3kPern-Da",
        "outputId": "45889a0c-9034-4139-ef77-668aedaf8b0e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-0.4516)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope it out to the entire dataset"
      ],
      "metadata": {
        "id": "LwZ3qVbR-vQW"
      }
    }
  ]
}